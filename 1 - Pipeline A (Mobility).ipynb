{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "441d1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using input file:\n",
      " - /Users/rafaelalbuquerque/Desktop/Splited Mobility Data/Mobility_Data.csv.gz\n",
      "\n",
      "[STEP 1/4] Reading header to build schema...\n",
      "[INFO] Columns detected: 61\n",
      "\n",
      "[STEP 2/4] Streaming ingestion and concatenation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c526564aea934bd5956018c550b88439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading chunks: 0chunk [00:00, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TOTAL rows written: 34,949,440\n",
      "\n",
      "[STEP 3/4] Computing QC statistics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b27c6de8674388a9eaa8795b46cec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QC pass (chunks): 0chunk [00:00, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4/4] Writing QC report...\n",
      "\n",
      "[DONE] Block A1 completed successfully.\n",
      " - Output data: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A1/mobility_raw_aug2024_concat.csv.gz\n",
      " - QC report : /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A1/mobility_qc_report.json\n",
      " - Runtime   : 3335.53 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline A — Mobility Data\n",
    "# Block A1 — Mobility Ingestion & Integrity Checks\n",
    "# ============================================================\n",
    "# Conversation: Portuguese | Code/comments: English\n",
    "# NEVER delete original columns. Only add.\n",
    "# Streaming / chunk-based ingestion (hardware-agnostic).\n",
    "# Real-time progress bars and logs are MANDATORY.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------\n",
    "T0 = time.time()\n",
    "\n",
    "# INPUT: reassembled gzip (DO NOT point to split parts)\n",
    "INPUT_FILE = os.path.expanduser(\n",
    "    \"~/Desktop/Splited Mobility Data/Mobility_Data.csv.gz\"\n",
    ")\n",
    "\n",
    "# OUTPUTS: official folder\n",
    "OUTPUT_DIR = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A1\"\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_CONCAT = os.path.join(\n",
    "    OUTPUT_DIR, \"mobility_raw_aug2024_concat.csv.gz\"\n",
    ")\n",
    "OUT_QC = os.path.join(\n",
    "    OUTPUT_DIR, \"mobility_qc_report.json\"\n",
    ")\n",
    "\n",
    "# IO / performance (safe on Mac M1/M2)\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# Canonical identifiers (DO NOT rename here)\n",
    "KEY_COL = \"store_id\"\n",
    "STATE_COL = \"State\"\n",
    "\n",
    "# Temporal components (existence check only)\n",
    "WEEK_COLS = [\n",
    "    \"unique_q1\", \"unique_q2\", \"unique_q3\", \"unique_q4\",\n",
    "    \"visits_q1\", \"visits_q2\", \"visits_q3\", \"visits_q4\"\n",
    "]\n",
    "\n",
    "# Metrics for range summaries (only if present)\n",
    "RANGE_METRICS = [\n",
    "    \"unique\", \"visits\", \"repeat_visitors\",\n",
    "    \"new_visitors\", \"dwell_time_mins\"\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Sanity checks\n",
    "# -----------------------------\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Input file not found: {INPUT_FILE}\"\n",
    "    )\n",
    "\n",
    "print(\"[INFO] Using input file:\")\n",
    "print(\" -\", INPUT_FILE)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Pass 1 — Schema union (headers only)\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 1/4] Reading header to build schema...\")\n",
    "\n",
    "# Read header only (fast)\n",
    "header_df = pd.read_csv(\n",
    "    INPUT_FILE,\n",
    "    compression=\"gzip\",\n",
    "    nrows=0\n",
    ")\n",
    "union_cols: List[str] = header_df.columns.tolist()\n",
    "\n",
    "print(f\"[INFO] Columns detected: {len(union_cols)}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Pass 2 — Streaming ingestion & write canonical raw\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 2/4] Streaming ingestion and concatenation...\")\n",
    "\n",
    "rows_total = 0\n",
    "\n",
    "# Ensure fresh write\n",
    "if os.path.exists(OUT_CONCAT):\n",
    "    os.remove(OUT_CONCAT)\n",
    "\n",
    "write_header = True\n",
    "\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(\n",
    "        INPUT_FILE,\n",
    "        compression=\"gzip\",\n",
    "        chunksize=CHUNKSIZE,\n",
    "        low_memory=False\n",
    "    ),\n",
    "    desc=\"Reading chunks\",\n",
    "    unit=\"chunk\"\n",
    "):\n",
    "    # Enforce schema order (no deletions)\n",
    "    for c in union_cols:\n",
    "        if c not in chunk.columns:\n",
    "            chunk[c] = np.nan\n",
    "    chunk = chunk[union_cols]\n",
    "\n",
    "    chunk.to_csv(\n",
    "        OUT_CONCAT,\n",
    "        mode=\"a\",\n",
    "        header=write_header,\n",
    "        index=False,\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "    write_header = False\n",
    "\n",
    "    n = len(chunk)\n",
    "    rows_total += n\n",
    "\n",
    "tqdm.write(f\"[INFO] TOTAL rows written: {rows_total:,}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Pass 3 — QC computations\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 3/4] Computing QC statistics...\")\n",
    "\n",
    "qc = {\n",
    "    \"block\": \"A1\",\n",
    "    \"pipeline\": \"A\",\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"input_file\": os.path.basename(INPUT_FILE),\n",
    "    \"rows_total\": int(rows_total),\n",
    "    \"columns_total\": len(union_cols),\n",
    "}\n",
    "\n",
    "unique_store_ids = set()\n",
    "state_counts: Dict[str, int] = {}\n",
    "missing_counts = {c: 0 for c in union_cols}\n",
    "value_ranges = {\n",
    "    m: {\"min\": None, \"max\": None}\n",
    "    for m in RANGE_METRICS if m in union_cols\n",
    "}\n",
    "\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(\n",
    "        OUT_CONCAT,\n",
    "        compression=\"gzip\",\n",
    "        chunksize=CHUNKSIZE\n",
    "    ),\n",
    "    desc=\"QC pass (chunks)\",\n",
    "    unit=\"chunk\"\n",
    "):\n",
    "    # Unique census tracts\n",
    "    if KEY_COL in chunk.columns:\n",
    "        unique_store_ids.update(\n",
    "            chunk[KEY_COL]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .unique()\n",
    "        )\n",
    "\n",
    "    # State coverage\n",
    "    if STATE_COL in chunk.columns:\n",
    "        vc = chunk[STATE_COL].value_counts(dropna=True)\n",
    "        for k, v in vc.items():\n",
    "            state_counts[k] = state_counts.get(k, 0) + int(v)\n",
    "\n",
    "    # Missingness\n",
    "    for c in union_cols:\n",
    "        missing_counts[c] += int(chunk[c].isna().sum())\n",
    "\n",
    "    # Ranges\n",
    "    for m in value_ranges:\n",
    "        s = pd.to_numeric(chunk[m], errors=\"coerce\")\n",
    "        if s.notna().any():\n",
    "            mn, mx = s.min(), s.max()\n",
    "            if (\n",
    "                value_ranges[m][\"min\"] is None\n",
    "                or mn < value_ranges[m][\"min\"]\n",
    "            ):\n",
    "                value_ranges[m][\"min\"] = float(mn)\n",
    "            if (\n",
    "                value_ranges[m][\"max\"] is None\n",
    "                or mx > value_ranges[m][\"max\"]\n",
    "            ):\n",
    "                value_ranges[m][\"max\"] = float(mx)\n",
    "\n",
    "qc[\"unique_census_tracts\"] = len(unique_store_ids)\n",
    "qc[\"coverage_by_state\"] = state_counts\n",
    "qc[\"missing_values\"] = {\n",
    "    k: int(v) for k, v in missing_counts.items()\n",
    "}\n",
    "qc[\"ranges\"] = value_ranges\n",
    "qc[\"temporal_components_present\"] = {\n",
    "    c: (c in union_cols) for c in WEEK_COLS\n",
    "}\n",
    "qc[\"runtime_seconds\"] = round(\n",
    "    time.time() - T0, 2\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save QC report\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 4/4] Writing QC report...\")\n",
    "\n",
    "with open(OUT_QC, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "print(\"\\n[DONE] Block A1 completed successfully.\")\n",
    "print(f\" - Output data: {OUT_CONCAT}\")\n",
    "print(f\" - QC report : {OUT_QC}\")\n",
    "print(f\" - Runtime   : {qc['runtime_seconds']} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b194512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Block A2 starting...\n",
      " - Input: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A1/mobility_raw_aug2024_concat.csv.gz\n",
      " - Output: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A2/mobility_aug2024_canonical.csv.gz\n",
      " - Dict  : /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A2/mobility_data_dictionary.csv\n",
      "[INFO] Columns detected in A1: 61\n",
      "[INFO] Using key column: store_id (alias will be ct_id)\n",
      "\n",
      "[STEP 1/2] Writing canonical dataset with aliases + derived vars...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affbf8101cfc4e55976240ea58bdc082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canonicalization pass (chunks): 0chunk [00:00, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Rows written to canonical dataset: 34,949,440\n",
      "\n",
      "[STEP 2/2] Building data dictionary...\n",
      "\n",
      "[DONE] Block A2 completed successfully.\n",
      " - Canonical data: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A2/mobility_aug2024_canonical.csv.gz\n",
      " - Data dictionary: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A2/mobility_data_dictionary.csv\n",
      " - Runtime: 4012.9 seconds\n",
      "[INFO] Added columns (count=26): ['ct_id', 'demo_age_range', 'demo_class', 'demo_gender', 'log1p_dwell_time_mins', 'log1p_new_visitor_q1', 'log1p_new_visitor_q2', 'log1p_new_visitor_q3', 'log1p_new_visitor_q4', 'log1p_new_visitors', 'log1p_repeat_q1', 'log1p_repeat_q2'] ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline A — Mobility Data\n",
    "# Block A2 — Mobility Canonicalization\n",
    "# ============================================================\n",
    "# Conversation: Portuguese | Code/comments: English\n",
    "# NEVER delete original columns. Only add.\n",
    "# Create canonical column aliases + derived variables for the paper.\n",
    "# Produce a data dictionary for Method (JM-proof).\n",
    "# Real-time progress bars and logs are MANDATORY.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------\n",
    "T0 = time.time()\n",
    "\n",
    "# INPUT from A1\n",
    "IN_A1 = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A1/mobility_raw_aug2024_concat.csv.gz\"\n",
    ")\n",
    "\n",
    "# OUTPUT folder for A2\n",
    "OUT_DIR = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A2\"\n",
    ")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_CANON = os.path.join(OUT_DIR, \"mobility_aug2024_canonical.csv.gz\")\n",
    "OUT_DICT = os.path.join(OUT_DIR, \"mobility_data_dictionary.csv\")\n",
    "\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# Core identifiers (original names)\n",
    "ORIG_KEY_COL = \"store_id\"   # do not rename; create alias\n",
    "ORIG_STATE_COL = \"State\"    # do not rename; create alias\n",
    "\n",
    "# Metrics to log-transform (if present)\n",
    "LOG_TARGETS = [\n",
    "    \"unique\", \"visits\", \"repeat_visitors\", \"new_visitors\",\n",
    "    \"dwell_time_mins\",\n",
    "    \"unique_q1\", \"unique_q2\", \"unique_q3\", \"unique_q4\",\n",
    "    \"visits_q1\", \"visits_q2\", \"visits_q3\", \"visits_q4\",\n",
    "    \"repeat_q1\", \"repeat_q2\", \"repeat_q3\", \"repeat_q4\",\n",
    "    \"new_visitor_q1\", \"new_visitor_q2\", \"new_visitor_q3\", \"new_visitor_q4\",\n",
    "]\n",
    "\n",
    "# Paper-first canonical alias map (ONLY ADD NEW COLUMNS, keep originals)\n",
    "# Notes:\n",
    "# - We do not drop or rename original columns.\n",
    "# - We create canonical alias columns with consistent snake_case naming.\n",
    "ALIAS_MAP = {\n",
    "    \"store_id\": \"ct_id\",                 # census tract id (alias)\n",
    "    \"month_part\": \"month_ref\",\n",
    "    \"State\": \"state_uf\",\n",
    "    \"demographics_gender\": \"demo_gender\",\n",
    "    \"demographics_age_range\": \"demo_age_range\",\n",
    "    \"demographics_class\": \"demo_class\",\n",
    "}\n",
    "\n",
    "# If your dataset sometimes has \"Store_id\" (capital S), we handle it safely\n",
    "ALT_KEY_CANDIDATES = [\"Store_id\", \"store_id\", \"Store_ID\", \"STORE_ID\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Sanity checks + read schema\n",
    "# -----------------------------\n",
    "if not os.path.exists(IN_A1):\n",
    "    raise FileNotFoundError(f\"A1 input not found: {IN_A1}\")\n",
    "\n",
    "print(\"[INFO] Block A2 starting...\")\n",
    "print(\" - Input:\", IN_A1)\n",
    "print(\" - Output:\", OUT_CANON)\n",
    "print(\" - Dict  :\", OUT_DICT)\n",
    "\n",
    "# Read header to get columns\n",
    "cols = pd.read_csv(IN_A1, compression=\"gzip\", nrows=0).columns.tolist()\n",
    "col_set = set(cols)\n",
    "print(f\"[INFO] Columns detected in A1: {len(cols)}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Resolve key column robustly (without renaming)\n",
    "# -----------------------------\n",
    "def resolve_key_column(columns: List[str]) -> str:\n",
    "    for c in ALT_KEY_CANDIDATES:\n",
    "        if c in columns:\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        f\"Could not find a census-tract identifier column among: {ALT_KEY_CANDIDATES}\"\n",
    "    )\n",
    "\n",
    "KEY_COL = resolve_key_column(cols)\n",
    "\n",
    "# Ensure alias map uses the resolved key name\n",
    "if KEY_COL != \"store_id\":\n",
    "    # Keep map for 'store_id' but also map actual found key\n",
    "    ALIAS_MAP[KEY_COL] = \"ct_id\"\n",
    "\n",
    "print(f\"[INFO] Using key column: {KEY_COL} (alias will be ct_id)\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Helper: safe numeric conversion\n",
    "# -----------------------------\n",
    "def to_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    # Fast, tolerant numeric conversion\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Streaming transform → write canonical dataset\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 1/2] Writing canonical dataset with aliases + derived vars...\")\n",
    "\n",
    "if os.path.exists(OUT_CANON):\n",
    "    os.remove(OUT_CANON)\n",
    "\n",
    "write_header = True\n",
    "rows_total = 0\n",
    "\n",
    "# Keep a running schema record (original + added)\n",
    "added_columns: List[str] = []\n",
    "\n",
    "# Precompute which alias columns are applicable\n",
    "applicable_aliases = {\n",
    "    orig: alias for orig, alias in ALIAS_MAP.items() if orig in col_set\n",
    "}\n",
    "# Make sure key alias exists\n",
    "if KEY_COL in col_set:\n",
    "    applicable_aliases[KEY_COL] = \"ct_id\"\n",
    "\n",
    "# Determine which log targets exist\n",
    "existing_log_targets = [c for c in LOG_TARGETS if c in col_set]\n",
    "\n",
    "# Derived columns we will add\n",
    "# - ct_id: alias for tract id (string)\n",
    "# - state_uf: alias for State\n",
    "# - log1p_<metric>: log(1 + x) for selected metrics (numeric, coerced)\n",
    "derived_log_cols = [f\"log1p_{c}\" for c in existing_log_targets]\n",
    "added_columns.extend(sorted(set(applicable_aliases.values())))\n",
    "added_columns.extend(derived_log_cols)\n",
    "\n",
    "# Stream through dataset\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(IN_A1, compression=\"gzip\", chunksize=CHUNKSIZE, low_memory=False),\n",
    "    desc=\"Canonicalization pass (chunks)\",\n",
    "    unit=\"chunk\"\n",
    "):\n",
    "    # 4.1) Add alias columns (do not drop originals)\n",
    "    for orig, alias in applicable_aliases.items():\n",
    "        if alias in chunk.columns:\n",
    "            continue\n",
    "        # Preserve IDs as string to avoid scientific notation / rounding issues\n",
    "        if orig == KEY_COL:\n",
    "            chunk[alias] = chunk[orig].astype(\"Int64\", errors=\"ignore\").astype(str)\n",
    "        else:\n",
    "            chunk[alias] = chunk[orig]\n",
    "\n",
    "    # 4.2) Add log1p derived columns (paper-friendly)\n",
    "    for c in existing_log_targets:\n",
    "        outc = f\"log1p_{c}\"\n",
    "        if outc in chunk.columns:\n",
    "            continue\n",
    "        x = to_numeric_series(chunk[c])\n",
    "        # log1p is stable for zeros and small values; negative values become NaN\n",
    "        x = x.where(x >= 0, np.nan)\n",
    "        chunk[outc] = np.log1p(x)\n",
    "\n",
    "    # 4.3) Add a lightweight QA flag column (paper-first, no cleaning)\n",
    "    # Example: rows with missing tract id are flagged (not removed).\n",
    "    if \"qa_missing_ct_id\" not in chunk.columns:\n",
    "        chunk[\"qa_missing_ct_id\"] = chunk[\"ct_id\"].isna() if \"ct_id\" in chunk.columns else True\n",
    "\n",
    "    # Write\n",
    "    chunk.to_csv(\n",
    "        OUT_CANON,\n",
    "        mode=\"a\",\n",
    "        header=write_header,\n",
    "        index=False,\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "    write_header = False\n",
    "\n",
    "    rows_total += len(chunk)\n",
    "\n",
    "tqdm.write(f\"[INFO] Rows written to canonical dataset: {rows_total:,}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Build data dictionary (Method-ready)\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 2/2] Building data dictionary...\")\n",
    "\n",
    "# Reload header from canonical file to capture added columns\n",
    "canon_cols = pd.read_csv(OUT_CANON, compression=\"gzip\", nrows=0).columns.tolist()\n",
    "\n",
    "# Type inference on a small sample (fast)\n",
    "sample = pd.read_csv(OUT_CANON, compression=\"gzip\", nrows=50_000, low_memory=False)\n",
    "\n",
    "def infer_format(series: pd.Series) -> str:\n",
    "    dt = series.dtype\n",
    "    if pd.api.types.is_integer_dtype(dt):\n",
    "        return \"INTEGER\"\n",
    "    if pd.api.types.is_float_dtype(dt):\n",
    "        return \"NUMERIC\"\n",
    "    if pd.api.types.is_bool_dtype(dt):\n",
    "        return \"BOOLEAN\"\n",
    "    if pd.api.types.is_datetime64_any_dtype(dt):\n",
    "        return \"DATETIME\"\n",
    "    return \"VARCHAR\"\n",
    "\n",
    "def label_for(col: str) -> str:\n",
    "    # Human-readable labels (paper-first)\n",
    "    if col == \"ct_id\":\n",
    "        return \"Census tract ID (alias)\"\n",
    "    if col == \"state_uf\":\n",
    "        return \"Brazilian state (alias)\"\n",
    "    if col.startswith(\"log1p_\"):\n",
    "        base = col.replace(\"log1p_\", \"\")\n",
    "        return f\"Log(1 + {base})\"\n",
    "    if col.startswith(\"qa_\"):\n",
    "        return \"Quality flag\"\n",
    "    # Fall back to original column name\n",
    "    return col\n",
    "\n",
    "def description_for(col: str) -> str:\n",
    "    # Crisp, defensible descriptions\n",
    "    if col == \"ct_id\":\n",
    "        return \"Alias for census tract code (original identifier preserved); stored as string to avoid rounding.\"\n",
    "    if col.startswith(\"log1p_\"):\n",
    "        base = col.replace(\"log1p_\", \"\")\n",
    "        return f\"Derived variable: natural log of (1 + {base}), with negatives treated as missing.\"\n",
    "    if col == \"qa_missing_ct_id\":\n",
    "        return \"Flag indicating missing census tract ID (no row removal; used for auditing).\"\n",
    "    if col in applicable_aliases.values():\n",
    "        return \"Alias column created for standardized naming; original column retained unchanged.\"\n",
    "    return \"\"\n",
    "\n",
    "def comment_for(col: str) -> str:\n",
    "    if col in col_set:\n",
    "        return \"Original column (unchanged)\"\n",
    "    return \"Derived/alias column (added)\"\n",
    "\n",
    "dict_rows = []\n",
    "for c in canon_cols:\n",
    "    dict_rows.append({\n",
    "        \"variable_name\": c,\n",
    "        \"variable_label\": label_for(c),\n",
    "        \"format\": infer_format(sample[c]) if c in sample.columns else \"\",\n",
    "        \"description\": description_for(c),\n",
    "        \"comments\": comment_for(c),\n",
    "    })\n",
    "\n",
    "dd = pd.DataFrame(dict_rows)\n",
    "\n",
    "# Save dictionary\n",
    "dd.to_csv(OUT_DICT, index=False)\n",
    "\n",
    "print(\"\\n[DONE] Block A2 completed successfully.\")\n",
    "print(f\" - Canonical data: {OUT_CANON}\")\n",
    "print(f\" - Data dictionary: {OUT_DICT}\")\n",
    "print(f\" - Runtime: {round(time.time() - T0, 2)} seconds\")\n",
    "print(f\"[INFO] Added columns (count={len(set(added_columns))}): {sorted(set(added_columns))[:12]}{' ...' if len(set(added_columns))>12 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae520e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Block A3 starting...\n",
      " - Input: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A2/mobility_aug2024_canonical.csv.gz\n",
      " - Output tract: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A3/mobility_by_tract_aug2024.csv.gz\n",
      " - Output coverage: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A3/mobility_coverage_by_state.csv.gz\n",
      "[INFO] Sum vars present: 20\n",
      "[INFO] Mean vars present: 1\n",
      "[INFO] State column: NOT AVAILABLE\n",
      "\n",
      "[STEP 1/3] Streaming aggregation to one row per ct_id...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df76e879d6c3419eabe3b5bbaea672d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregation pass (chunks): 0chunk [00:00, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tracts aggregated (unique ct_id): 436,868\n",
      "\n",
      "[STEP 2/3] Materializing tract-level dataset + stability checks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8bef53f60842269f98da97f56bf3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building output rows:   0%|          | 0/436868 [00:00<?, ?tract/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tract-level rows: 436,868\n",
      "[INFO] Columns in tract dataset: 30\n",
      "\n",
      "[STEP 3/3] Computing coverage by state...\n",
      "\n",
      "[DONE] Block A3 completed successfully.\n",
      " - mobility_by_tract_aug2024.csv.gz: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A3/mobility_by_tract_aug2024.csv.gz\n",
      " - mobility_coverage_by_state.csv.gz: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A3/mobility_coverage_by_state.csv.gz\n",
      " - Runtime: 783.23 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline A — Mobility Data\n",
    "# Block A3 — Mobility Aggregation to Base Spatial Unit (Census Tract)\n",
    "# ============================================================\n",
    "# Conversation: Portuguese | Code/comments: English\n",
    "# NEVER delete original columns. Only add.\n",
    "# Aggregate to one row per census tract (ct_id).\n",
    "# Real-time progress bars and logs are MANDATORY.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------\n",
    "T0 = time.time()\n",
    "\n",
    "IN_A2 = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A2/mobility_aug2024_canonical.csv.gz\"\n",
    ")\n",
    "\n",
    "OUT_DIR = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A3\"\n",
    ")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_TRACT = os.path.join(OUT_DIR, \"mobility_by_tract_aug2024.csv.gz\")\n",
    "OUT_COV = os.path.join(OUT_DIR, \"mobility_coverage_by_state.csv.gz\")\n",
    "\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# Canonical columns created in A2\n",
    "CT_COL = \"ct_id\"\n",
    "STATE_COL = \"state_uf\"  # alias may exist; fallback handled below\n",
    "\n",
    "# Core mobility fields (if present)\n",
    "SUM_VARS = [\n",
    "    \"unique\", \"visits\", \"repeat_visitors\", \"new_visitors\",\n",
    "    \"unique_q1\", \"unique_q2\", \"unique_q3\", \"unique_q4\",\n",
    "    \"visits_q1\", \"visits_q2\", \"visits_q3\", \"visits_q4\",\n",
    "    \"repeat_q1\", \"repeat_q2\", \"repeat_q3\", \"repeat_q4\",\n",
    "    \"new_visitor_q1\", \"new_visitor_q2\", \"new_visitor_q3\", \"new_visitor_q4\",\n",
    "]\n",
    "MEAN_VARS = [\"dwell_time_mins\"]\n",
    "\n",
    "# Stability checks\n",
    "WEEK_UNIQUE = [\"unique_q1\", \"unique_q2\", \"unique_q3\", \"unique_q4\"]\n",
    "WEEK_VISITS = [\"visits_q1\", \"visits_q2\", \"visits_q3\", \"visits_q4\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Sanity checks\n",
    "# -----------------------------\n",
    "if not os.path.exists(IN_A2):\n",
    "    raise FileNotFoundError(f\"A2 input not found: {IN_A2}\")\n",
    "\n",
    "print(\"[INFO] Block A3 starting...\")\n",
    "print(\" - Input:\", IN_A2)\n",
    "print(\" - Output tract:\", OUT_TRACT)\n",
    "print(\" - Output coverage:\", OUT_COV)\n",
    "\n",
    "cols = pd.read_csv(IN_A2, compression=\"gzip\", nrows=0).columns.tolist()\n",
    "col_set = set(cols)\n",
    "\n",
    "if CT_COL not in col_set:\n",
    "    raise KeyError(f\"Required column not found in A2: {CT_COL}\")\n",
    "\n",
    "# Resolve state column (prefer alias; fallback to original)\n",
    "if STATE_COL not in col_set:\n",
    "    if \"State\" in col_set:\n",
    "        STATE_COL = \"State\"\n",
    "    else:\n",
    "        STATE_COL = None\n",
    "\n",
    "present_sum_vars = [c for c in SUM_VARS if c in col_set]\n",
    "present_mean_vars = [c for c in MEAN_VARS if c in col_set]\n",
    "\n",
    "print(f\"[INFO] Sum vars present: {len(present_sum_vars)}\")\n",
    "print(f\"[INFO] Mean vars present: {len(present_mean_vars)}\")\n",
    "print(f\"[INFO] State column: {STATE_COL if STATE_COL else 'NOT AVAILABLE'}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Streaming aggregation (dictionary-of-aggregates)\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 1/3] Streaming aggregation to one row per ct_id...\")\n",
    "\n",
    "# Aggregator stores partial sums/counts per tract\n",
    "agg_sum: Dict[str, Dict[str, float]] = {}\n",
    "agg_cnt: Dict[str, Dict[str, int]] = {}  # for mean vars: counts\n",
    "agg_state: Dict[str, str] = {}           # tract -> state (first non-null)\n",
    "\n",
    "def get_or_init(d: Dict, key: str, init_factory):\n",
    "    if key not in d:\n",
    "        d[key] = init_factory()\n",
    "    return d[key]\n",
    "\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(IN_A2, compression=\"gzip\", chunksize=CHUNKSIZE, low_memory=False),\n",
    "    desc=\"Aggregation pass (chunks)\",\n",
    "    unit=\"chunk\"\n",
    "):\n",
    "    # Keep only necessary columns for aggregation to reduce overhead\n",
    "    keep_cols = [CT_COL] + present_sum_vars + present_mean_vars\n",
    "    if STATE_COL:\n",
    "        keep_cols.append(STATE_COL)\n",
    "    sub = chunk[keep_cols].copy()\n",
    "\n",
    "    # Ensure ct_id is string for dict keys\n",
    "    sub[CT_COL] = sub[CT_COL].astype(str)\n",
    "\n",
    "    # Convert numeric vars safely\n",
    "    for c in present_sum_vars + present_mean_vars:\n",
    "        sub[c] = pd.to_numeric(sub[c], errors=\"coerce\")\n",
    "\n",
    "    # Group by ct_id inside chunk (fast)\n",
    "    gb = sub.groupby(CT_COL, dropna=False)\n",
    "\n",
    "    # Sum vars\n",
    "    if present_sum_vars:\n",
    "        sums = gb[present_sum_vars].sum(min_count=1)\n",
    "    else:\n",
    "        sums = None\n",
    "\n",
    "    # Mean vars tracked via sum + count\n",
    "    if present_mean_vars:\n",
    "        mean_sums = gb[present_mean_vars].sum(min_count=1)\n",
    "        mean_cnts = gb[present_mean_vars].count()\n",
    "    else:\n",
    "        mean_sums = mean_cnts = None\n",
    "\n",
    "    # State (first non-null)\n",
    "    if STATE_COL:\n",
    "        st = gb[STATE_COL].agg(lambda x: x.dropna().iloc[0] if x.dropna().shape[0] else np.nan)\n",
    "\n",
    "    # Merge into global dictionaries\n",
    "    idx = sums.index if sums is not None else (mean_sums.index if mean_sums is not None else st.index)\n",
    "\n",
    "    for ct in idx:\n",
    "        # init containers\n",
    "        sdict = get_or_init(agg_sum, ct, lambda: {})\n",
    "        cdict = get_or_init(agg_cnt, ct, lambda: {})\n",
    "\n",
    "        if sums is not None:\n",
    "            row = sums.loc[ct]\n",
    "            for c in present_sum_vars:\n",
    "                v = row[c]\n",
    "                if pd.notna(v):\n",
    "                    sdict[c] = sdict.get(c, 0.0) + float(v)\n",
    "\n",
    "        if mean_sums is not None:\n",
    "            row_s = mean_sums.loc[ct]\n",
    "            row_c = mean_cnts.loc[ct]\n",
    "            for c in present_mean_vars:\n",
    "                vs = row_s[c]\n",
    "                vc = row_c[c]\n",
    "                if pd.notna(vs) and vc > 0:\n",
    "                    # store running sum and count\n",
    "                    sdict[f\"__sum_{c}\"] = sdict.get(f\"__sum_{c}\", 0.0) + float(vs)\n",
    "                    cdict[c] = cdict.get(c, 0) + int(vc)\n",
    "\n",
    "        if STATE_COL:\n",
    "            v = st.loc[ct]\n",
    "            if ct not in agg_state and pd.notna(v):\n",
    "                agg_state[ct] = str(v)\n",
    "\n",
    "tqdm.write(f\"[INFO] Tracts aggregated (unique ct_id): {len(agg_sum):,}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Materialize aggregated dataframe + stability checks\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 2/3] Materializing tract-level dataset + stability checks...\")\n",
    "\n",
    "rows = []\n",
    "for ct, sdict in tqdm(agg_sum.items(), desc=\"Building output rows\", unit=\"tract\"):\n",
    "    row = {CT_COL: ct}\n",
    "\n",
    "    # Attach state if available\n",
    "    if STATE_COL:\n",
    "        row[\"state_uf\"] = agg_state.get(ct, np.nan)\n",
    "\n",
    "    # Add sums\n",
    "    for c in present_sum_vars:\n",
    "        row[c] = sdict.get(c, np.nan)\n",
    "\n",
    "    # Compute means from running sums/counts\n",
    "    for c in present_mean_vars:\n",
    "        s = sdict.get(f\"__sum_{c}\", np.nan)\n",
    "        n = agg_cnt.get(ct, {}).get(c, 0)\n",
    "        row[c] = (s / n) if (pd.notna(s) and n > 0) else np.nan\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "tract_df = pd.DataFrame(rows)\n",
    "\n",
    "# Add stability checks (no deletion; add new columns)\n",
    "def safe_cv(vals: pd.Series) -> float:\n",
    "    x = pd.to_numeric(vals, errors=\"coerce\")\n",
    "    x = x.dropna()\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    mu = x.mean()\n",
    "    sd = x.std(ddof=1)\n",
    "    return float(sd / mu) if (mu and mu > 0) else np.nan\n",
    "\n",
    "# Unique stability\n",
    "if all(c in tract_df.columns for c in WEEK_UNIQUE):\n",
    "    tract_df[\"unique_week_mean\"] = tract_df[WEEK_UNIQUE].mean(axis=1, skipna=True)\n",
    "    tract_df[\"unique_week_cv\"] = tract_df[WEEK_UNIQUE].apply(safe_cv, axis=1)\n",
    "\n",
    "# Visits stability\n",
    "if all(c in tract_df.columns for c in WEEK_VISITS):\n",
    "    tract_df[\"visits_week_mean\"] = tract_df[WEEK_VISITS].mean(axis=1, skipna=True)\n",
    "    tract_df[\"visits_week_cv\"] = tract_df[WEEK_VISITS].apply(safe_cv, axis=1)\n",
    "\n",
    "# Sanity: if totals exist, compare against sum of weeks (add diagnostics)\n",
    "if \"unique\" in tract_df.columns and all(c in tract_df.columns for c in WEEK_UNIQUE):\n",
    "    tract_df[\"unique_weeks_sum\"] = tract_df[WEEK_UNIQUE].sum(axis=1, skipna=True)\n",
    "    tract_df[\"unique_total_minus_weeks\"] = tract_df[\"unique\"] - tract_df[\"unique_weeks_sum\"]\n",
    "\n",
    "if \"visits\" in tract_df.columns and all(c in tract_df.columns for c in WEEK_VISITS):\n",
    "    tract_df[\"visits_weeks_sum\"] = tract_df[WEEK_VISITS].sum(axis=1, skipna=True)\n",
    "    tract_df[\"visits_total_minus_weeks\"] = tract_df[\"visits\"] - tract_df[\"visits_weeks_sum\"]\n",
    "\n",
    "print(f\"[INFO] Tract-level rows: {len(tract_df):,}\")\n",
    "print(f\"[INFO] Columns in tract dataset: {tract_df.shape[1]}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Coverage by state (UF)\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 3/3] Computing coverage by state...\")\n",
    "\n",
    "if \"state_uf\" in tract_df.columns:\n",
    "    cov = tract_df.groupby(\"state_uf\", dropna=False).agg(\n",
    "        tracts=(\"ct_id\", \"nunique\"),\n",
    "        total_visits=(\"visits\", \"sum\") if \"visits\" in tract_df.columns else (\"ct_id\", \"size\"),\n",
    "        total_unique=(\"unique\", \"sum\") if \"unique\" in tract_df.columns else (\"ct_id\", \"size\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Sort for readability\n",
    "    cov = cov.sort_values(\"tracts\", ascending=False)\n",
    "else:\n",
    "    cov = pd.DataFrame({\"state_uf\": [], \"tracts\": [], \"total_visits\": [], \"total_unique\": []})\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save outputs\n",
    "# -----------------------------\n",
    "tract_df.to_csv(OUT_TRACT, index=False, compression=\"gzip\")\n",
    "cov.to_csv(OUT_COV, index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"\\n[DONE] Block A3 completed successfully.\")\n",
    "print(f\" - mobility_by_tract_aug2024.csv.gz: {OUT_TRACT}\")\n",
    "print(f\" - mobility_coverage_by_state.csv.gz: {OUT_COV}\")\n",
    "print(f\" - Runtime: {round(time.time() - T0, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75848535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading tract-level mobility file...\n",
      "[INFO] Deriving state_uf from ct_id prefix...\n",
      "[DONE] Coverage by state written successfully.\n",
      " - Output: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A3/mobility_coverage_by_state.csv.gz\n",
      "\n",
      "[TOP 10] states by number of tracts:\n",
      "state_uf  tracts  total_visits  total_unique\n",
      "      SP  102062  1.159421e+11  3.107754e+10\n",
      "      MG   49889  2.678671e+10  5.703628e+09\n",
      "      RJ   41279  3.307024e+10  8.970878e+09\n",
      "      BA   29569  2.382584e+10  5.108769e+09\n",
      "      RS   24815  1.841142e+10  4.325322e+09\n",
      "      PR   22875  1.861888e+10  4.388147e+09\n",
      "      CE   19231  8.191152e+09  1.585258e+09\n",
      "      PE   17242  2.705244e+10  6.168713e+09\n",
      "      SC   15204  1.015107e+10  2.196125e+09\n",
      "      PA   12868  9.880079e+09  1.811262e+09\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline A — Mobility Data\n",
    "# Block A3.1 — Coverage by UF derived from IBGE tract code (ct_id)\n",
    "# ============================================================\n",
    "# Conversation: Portuguese | Code/comments: English\n",
    "# NEVER delete original columns. Only add.\n",
    "# Derive state_uf from IBGE census tract code prefix.\n",
    "# Real-time progress bars and logs are MANDATORY.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IN_TRACT = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A3/mobility_by_tract_aug2024.csv.gz\"\n",
    ")\n",
    "\n",
    "OUT_COV = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A3/mobility_coverage_by_state.csv.gz\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(IN_TRACT):\n",
    "    raise FileNotFoundError(f\"Tract-level file not found: {IN_TRACT}\")\n",
    "\n",
    "print(\"[INFO] Reading tract-level mobility file...\")\n",
    "df = pd.read_csv(IN_TRACT, compression=\"gzip\")\n",
    "\n",
    "if \"ct_id\" not in df.columns:\n",
    "    raise KeyError(\"ct_id not found in tract-level dataset.\")\n",
    "\n",
    "# IBGE UF code mapping (2-digit)\n",
    "UF_MAP = {\n",
    "    \"11\": \"RO\", \"12\": \"AC\", \"13\": \"AM\", \"14\": \"RR\", \"15\": \"PA\", \"16\": \"AP\", \"17\": \"TO\",\n",
    "    \"21\": \"MA\", \"22\": \"PI\", \"23\": \"CE\", \"24\": \"RN\", \"25\": \"PB\", \"26\": \"PE\", \"27\": \"AL\", \"28\": \"SE\", \"29\": \"BA\",\n",
    "    \"31\": \"MG\", \"32\": \"ES\", \"33\": \"RJ\", \"35\": \"SP\",\n",
    "    \"41\": \"PR\", \"42\": \"SC\", \"43\": \"RS\",\n",
    "    \"50\": \"MS\", \"51\": \"MT\", \"52\": \"GO\", \"53\": \"DF\"\n",
    "}\n",
    "\n",
    "print(\"[INFO] Deriving state_uf from ct_id prefix...\")\n",
    "\n",
    "# Ensure ct_id is string, extract first 2 digits as UF code\n",
    "df[\"ct_id\"] = df[\"ct_id\"].astype(str)\n",
    "df[\"uf_code\"] = df[\"ct_id\"].str.slice(0, 2)\n",
    "df[\"state_uf\"] = df[\"uf_code\"].map(UF_MAP)\n",
    "\n",
    "# Coverage aggregation\n",
    "agg_spec = {\"tracts\": (\"ct_id\", \"nunique\")}\n",
    "if \"visits\" in df.columns:\n",
    "    agg_spec[\"total_visits\"] = (\"visits\", \"sum\")\n",
    "if \"unique\" in df.columns:\n",
    "    agg_spec[\"total_unique\"] = (\"unique\", \"sum\")\n",
    "\n",
    "cov = (\n",
    "    df.groupby(\"state_uf\", dropna=False)\n",
    "      .agg(**agg_spec)\n",
    "      .reset_index()\n",
    "      .sort_values(\"tracts\", ascending=False)\n",
    ")\n",
    "\n",
    "cov.to_csv(OUT_COV, index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"[DONE] Coverage by state written successfully.\")\n",
    "print(\" - Output:\", OUT_COV)\n",
    "print(\"\\n[TOP 10] states by number of tracts:\")\n",
    "print(cov.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c2ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Block A4 starting...\n",
      " - Input : /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A3/mobility_by_tract_aug2024.csv.gz\n",
      " - Output: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mobility_by_tract_aug2024_with_mii.csv.gz\n",
      " - QC    : /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mii_qc_report.json\n",
      "[INFO] Loaded rows: 436,868 | cols: 30\n",
      "\n",
      "[STEP 1/4] Preparing MII components...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e370f49faf2a477997dbb9f6c31e55a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Log1p transforms:   0%|          | 0/4 [00:00<?, ?var/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Final component set used for indexing:\n",
      " - dwell_time_mins\n",
      " - log1p_visits_A4\n",
      " - log1p_unique_A4\n",
      " - log1p_repeat_visitors_A4\n",
      " - log1p_new_visitors_A4\n",
      " - stability_visits_week_cv_A4\n",
      " - stability_unique_week_cv_A4\n",
      "\n",
      "[STEP 2/4] Standardizing components (z-scores) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7a7f1d685448b9b163f073a4efd51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Z-scoring:   0%|          | 0/7 [00:00<?, ?var/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PCA fit will use complete cases: 426,204 / 436,868\n",
      "\n",
      "[STEP 3/4] Building QC report...\n",
      "\n",
      "[STEP 4/4] Writing outputs...\n",
      "\n",
      "[DONE] Block A4 completed successfully.\n",
      " - Data with MII: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mobility_by_tract_aug2024_with_mii.csv.gz\n",
      " - QC report   : /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mii_qc_report.json\n",
      " - Runtime     : 32.46 seconds\n",
      "[INFO] PCA EVR (PC1): 0.6165\n",
      "[INFO] Corr(z-mean, PCA1): 0.9516\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline A — Mobility Data\n",
    "# Block A4 — Mobility Infrastructure Index (MII)\n",
    "# ============================================================\n",
    "# Conversation: Portuguese | Code/comments: English\n",
    "# NEVER delete original columns. Only add.\n",
    "# Build MII at the census tract level using defensible components.\n",
    "# Provide both PCA-based index and simple z-score composite as robustness.\n",
    "# Real-time progress bars and logs are MANDATORY.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------\n",
    "T0 = time.time()\n",
    "\n",
    "IN_A3 = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A3/mobility_by_tract_aug2024.csv.gz\"\n",
    ")\n",
    "\n",
    "OUT_DIR = os.path.expanduser(\n",
    "    \"~/Desktop/Output Pipeline A (Mobility)/A4\"\n",
    ")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_WITH_MII = os.path.join(OUT_DIR, \"mobility_by_tract_aug2024_with_mii.csv.gz\")\n",
    "OUT_QC = os.path.join(OUT_DIR, \"mii_qc_report.json\")\n",
    "\n",
    "CT_COL = \"ct_id\"\n",
    "\n",
    "# MII components:\n",
    "# - intensity: visits, unique\n",
    "# - absorption: dwell_time_mins, repeat_visitors\n",
    "# - renewal: new_visitors\n",
    "# - stability: negative CV (lower CV = more stable = higher infrastructure)\n",
    "BASE_COMPONENTS = [\n",
    "    \"visits\",\n",
    "    \"unique\",\n",
    "    \"repeat_visitors\",\n",
    "    \"new_visitors\",\n",
    "    \"dwell_time_mins\",\n",
    "    \"visits_week_cv\",\n",
    "    \"unique_week_cv\",\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "EPS = 1e-9\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load tract-level dataset\n",
    "# -----------------------------\n",
    "if not os.path.exists(IN_A3):\n",
    "    raise FileNotFoundError(f\"A3 input not found: {IN_A3}\")\n",
    "\n",
    "print(\"[INFO] Block A4 starting...\")\n",
    "print(\" - Input :\", IN_A3)\n",
    "print(\" - Output:\", OUT_WITH_MII)\n",
    "print(\" - QC    :\", OUT_QC)\n",
    "\n",
    "df = pd.read_csv(IN_A3, compression=\"gzip\")\n",
    "print(f\"[INFO] Loaded rows: {len(df):,} | cols: {df.shape[1]:,}\")\n",
    "\n",
    "if CT_COL not in df.columns:\n",
    "    raise KeyError(f\"Required column not found: {CT_COL}\")\n",
    "\n",
    "# Ensure ID is string (audit-safe)\n",
    "df[CT_COL] = df[CT_COL].astype(str)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Prepare component matrix (paper-first, defensible)\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 1/4] Preparing MII components...\")\n",
    "\n",
    "present = [c for c in BASE_COMPONENTS if c in df.columns]\n",
    "missing = [c for c in BASE_COMPONENTS if c not in df.columns]\n",
    "\n",
    "if missing:\n",
    "    print(\"[WARN] Missing MII components (will be ignored):\", missing)\n",
    "\n",
    "if len(present) < 4:\n",
    "    raise ValueError(\n",
    "        f\"Too few components present to build a defensible index. Present: {present}\"\n",
    "    )\n",
    "\n",
    "# Convert components to numeric\n",
    "for c in present:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Log1p transform for flow variables (intensity/renewal/persistence) to reduce skew\n",
    "# Note: dwell time is already an average; keep in level but z-score it.\n",
    "log1p_vars = [c for c in [\"visits\", \"unique\", \"repeat_visitors\", \"new_visitors\"] if c in present]\n",
    "for c in tqdm(log1p_vars, desc=\"Log1p transforms\", unit=\"var\"):\n",
    "    df[f\"log1p_{c}_A4\"] = np.log1p(df[c].where(df[c] >= 0, np.nan))\n",
    "present_augmented = present.copy()\n",
    "for c in log1p_vars:\n",
    "    present_augmented.remove(c)\n",
    "    present_augmented.append(f\"log1p_{c}_A4\")\n",
    "\n",
    "# Stability: convert CV to \"stability score\" (higher = more stable)\n",
    "# We use negative CV and then z-score it; this is monotonic and transparent.\n",
    "stability_vars = []\n",
    "for cv in [\"visits_week_cv\", \"unique_week_cv\"]:\n",
    "    if cv in present_augmented:\n",
    "        df[f\"stability_{cv}_A4\"] = -1.0 * df[cv]\n",
    "        stability_vars.append(f\"stability_{cv}_A4\")\n",
    "        present_augmented.remove(cv)\n",
    "        present_augmented.append(f\"stability_{cv}_A4\")\n",
    "\n",
    "print(\"[INFO] Final component set used for indexing:\")\n",
    "for c in present_augmented:\n",
    "    print(\" -\", c)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Standardize (z-scores) and compute two indices\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 2/4] Standardizing components (z-scores) ...\")\n",
    "\n",
    "Z_COLS = []\n",
    "for c in tqdm(present_augmented, desc=\"Z-scoring\", unit=\"var\"):\n",
    "    x = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    mu = x.mean(skipna=True)\n",
    "    sd = x.std(skipna=True, ddof=1)\n",
    "    zcol = f\"z_{c}\"\n",
    "    if sd is None or np.isnan(sd) or sd < EPS:\n",
    "        df[zcol] = np.nan\n",
    "    else:\n",
    "        df[zcol] = (x - mu) / sd\n",
    "    Z_COLS.append(zcol)\n",
    "\n",
    "# Matrix for PCA (rows with any NaNs are dropped ONLY for PCA fit; we still keep all rows in output)\n",
    "Z = df[Z_COLS]\n",
    "rows_before = len(df)\n",
    "Z_complete = Z.dropna()\n",
    "rows_used = len(Z_complete)\n",
    "\n",
    "print(f\"[INFO] PCA fit will use complete cases: {rows_used:,} / {rows_before:,}\")\n",
    "\n",
    "if rows_used < 10_000:\n",
    "    print(\"[WARN] Low number of complete cases for PCA (unexpected). PCA may be unstable.\")\n",
    "\n",
    "# 3.1) Simple composite (mean of z-scores)\n",
    "df[\"mii_zmean_A4\"] = Z.mean(axis=1, skipna=False)  # strict: if any component missing -> NaN\n",
    "\n",
    "# 3.2) PCA-based index (1st component on complete cases)\n",
    "pca = PCA(n_components=1, random_state=42)\n",
    "pca.fit(Z_complete.values)\n",
    "\n",
    "# Scores for all rows (NaN where missing)\n",
    "df[\"mii_pca1_A4\"] = np.nan\n",
    "df.loc[Z_complete.index, \"mii_pca1_A4\"] = pca.transform(Z_complete.values).reshape(-1)\n",
    "\n",
    "# Align sign so that higher values correspond to \"more mobility infrastructure\"\n",
    "# We enforce positive correlation with log1p_visits (if present).\n",
    "sign_anchor = \"z_log1p_visits_A4\" if \"z_log1p_visits_A4\" in df.columns else None\n",
    "if sign_anchor and df.loc[Z_complete.index, sign_anchor].notna().any():\n",
    "    corr = np.corrcoef(\n",
    "        df.loc[Z_complete.index, \"mii_pca1_A4\"].values,\n",
    "        df.loc[Z_complete.index, sign_anchor].values\n",
    "    )[0, 1]\n",
    "    if np.isfinite(corr) and corr < 0:\n",
    "        df[\"mii_pca1_A4\"] *= -1.0\n",
    "\n",
    "# Optional: standardized versions of indices for interpretability\n",
    "for idx_col in [\"mii_zmean_A4\", \"mii_pca1_A4\"]:\n",
    "    x = df[idx_col]\n",
    "    mu = x.mean(skipna=True)\n",
    "    sd = x.std(skipna=True, ddof=1)\n",
    "    df[f\"z_{idx_col}\"] = (x - mu) / sd if (sd and np.isfinite(sd) and sd > EPS) else np.nan\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) QC report (audit-ready)\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 3/4] Building QC report...\")\n",
    "\n",
    "qc: Dict = {\n",
    "    \"block\": \"A4\",\n",
    "    \"pipeline\": \"A\",\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"input_file\": os.path.basename(IN_A3),\n",
    "    \"rows_total\": int(len(df)),\n",
    "    \"component_base_requested\": BASE_COMPONENTS,\n",
    "    \"components_present\": present,\n",
    "    \"components_used_final\": present_augmented,\n",
    "    \"z_columns\": Z_COLS,\n",
    "    \"pca_complete_cases_used\": int(rows_used),\n",
    "    \"pca_explained_variance_ratio_pc1\": float(pca.explained_variance_ratio_[0]),\n",
    "    \"pca_loadings_pc1\": {\n",
    "        Z_COLS[i]: float(pca.components_[0][i]) for i in range(len(Z_COLS))\n",
    "    },\n",
    "    \"missingness\": {\n",
    "        \"mii_zmean_A4_na\": int(df[\"mii_zmean_A4\"].isna().sum()),\n",
    "        \"mii_pca1_A4_na\": int(df[\"mii_pca1_A4\"].isna().sum()),\n",
    "    },\n",
    "    \"runtime_seconds\": None,\n",
    "}\n",
    "\n",
    "# Distribution sanity checks (no plots, just summary)\n",
    "def summarize(series: pd.Series) -> Dict[str, float]:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    return {\n",
    "        \"mean\": float(s.mean()),\n",
    "        \"std\": float(s.std(ddof=1)),\n",
    "        \"p01\": float(s.quantile(0.01)),\n",
    "        \"p05\": float(s.quantile(0.05)),\n",
    "        \"p50\": float(s.quantile(0.50)),\n",
    "        \"p95\": float(s.quantile(0.95)),\n",
    "        \"p99\": float(s.quantile(0.99)),\n",
    "        \"min\": float(s.min()),\n",
    "        \"max\": float(s.max()),\n",
    "        \"n\": int(s.shape[0]),\n",
    "    }\n",
    "\n",
    "qc[\"index_summaries\"] = {\n",
    "    \"mii_zmean_A4\": summarize(df[\"mii_zmean_A4\"]),\n",
    "    \"mii_pca1_A4\": summarize(df[\"mii_pca1_A4\"]),\n",
    "    \"z_mii_zmean_A4\": summarize(df[\"z_mii_zmean_A4\"]),\n",
    "    \"z_mii_pca1_A4\": summarize(df[\"z_mii_pca1_A4\"]),\n",
    "}\n",
    "\n",
    "# Correlations between indices (complete overlap only)\n",
    "overlap = df[[\"mii_zmean_A4\", \"mii_pca1_A4\"]].dropna()\n",
    "if len(overlap) > 0:\n",
    "    qc[\"corr_mii_zmean_vs_pca1\"] = float(overlap.corr().iloc[0, 1])\n",
    "else:\n",
    "    qc[\"corr_mii_zmean_vs_pca1\"] = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Write outputs\n",
    "# -----------------------------\n",
    "print(\"\\n[STEP 4/4] Writing outputs...\")\n",
    "\n",
    "df.to_csv(OUT_WITH_MII, index=False, compression=\"gzip\")\n",
    "\n",
    "qc[\"runtime_seconds\"] = round(time.time() - T0, 2)\n",
    "with open(OUT_QC, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "print(\"\\n[DONE] Block A4 completed successfully.\")\n",
    "print(f\" - Data with MII: {OUT_WITH_MII}\")\n",
    "print(f\" - QC report   : {OUT_QC}\")\n",
    "print(f\" - Runtime     : {qc['runtime_seconds']} seconds\")\n",
    "print(f\"[INFO] PCA EVR (PC1): {qc['pca_explained_variance_ratio_pc1']:.4f}\")\n",
    "if qc.get(\"corr_mii_zmean_vs_pca1\") is not None:\n",
    "    print(f\"[INFO] Corr(z-mean, PCA1): {qc['corr_mii_zmean_vs_pca1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bebc052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Block A4b starting — MII materialization\n",
      " - Input : /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mobility_by_tract_aug2024_with_mii.csv.gz\n",
      " - Output: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\n",
      "[DONE] Block A4b completed successfully.\n",
      " - Output saved: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\n",
      " - Runtime     : 34.14s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PIPELINE A — BLOCK A4b\n",
    "# Mobility Infrastructure Index (MII) — Materialization\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "INPUT = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline A (Mobility)/A4/\"\n",
    "    \"mobility_by_tract_aug2024_with_mii.csv.gz\"\n",
    ")\n",
    "\n",
    "OUTPUT = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline A (Mobility)/A4/\"\n",
    "    \"mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\"\n",
    ")\n",
    "\n",
    "QC_REPORT = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline A (Mobility)/A4/\"\n",
    "    \"mii_materialization_qc.json\"\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(\"[INFO] Block A4b starting — MII materialization\")\n",
    "print(f\" - Input : {INPUT}\")\n",
    "print(f\" - Output: {OUTPUT}\")\n",
    "\n",
    "df = pd.read_csv(INPUT)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# COMPONENTS USED IN A4 (explicit & frozen)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "COMPONENTS = [\n",
    "    \"dwell_time_mins\",\n",
    "    \"log1p_visits_A4\",\n",
    "    \"log1p_unique_A4\",\n",
    "    \"log1p_repeat_visitors_A4\",\n",
    "    \"log1p_new_visitors_A4\",\n",
    "    \"stability_visits_week_cv_A4\",\n",
    "    \"stability_unique_week_cv_A4\"\n",
    "]\n",
    "\n",
    "missing = [c for c in COMPONENTS if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing MII components: {missing}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STANDARDIZATION (Z-SCORES)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "z = df[COMPONENTS].apply(\n",
    "    lambda x: (x - x.mean()) / x.std(ddof=0)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MII CONSTRUCTION (Z-MEAN)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df[\"mii\"] = z.mean(axis=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# QC\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "qc = {\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"components\": COMPONENTS,\n",
    "    \"method\": \"z-score mean (validated against PCA in Block A4)\",\n",
    "    \"mii_mean\": float(df[\"mii\"].mean()),\n",
    "    \"mii_std\": float(df[\"mii\"].std()),\n",
    "}\n",
    "\n",
    "df.to_csv(OUTPUT, index=False, compression=\"gzip\")\n",
    "\n",
    "with open(QC_REPORT, \"w\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(\"[DONE] Block A4b completed successfully.\")\n",
    "print(f\" - Output saved: {OUTPUT}\")\n",
    "print(f\" - Runtime     : {elapsed:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
