{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9cc548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Pipeline V — Block V1: Distinctiveness (MII ≠ Volume)\n",
      "Run ID: 20260103_185556\n",
      "Input file: /Users/rafaelalbuquerque/Desktop/Output Pipeline A (Mobility)/A4/mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\n",
      "Output dir: /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/\n",
      "----------------------------------------------------------------------\n",
      "Rows loaded: 436,868\n",
      "Columns loaded: 48\n",
      "----------------------------------------------------------------------\n",
      "Resolved column mapping (auto):\n",
      "  - ct_id_col: ct_id  | meta={'rule': 'exact', 'match': 'ct_id'}\n",
      "  - mii_col: mii  | meta={'rule': 'exact', 'match': 'mii'}\n",
      "  - visits_col: visits  | meta={'rule': 'exact', 'match': 'visits'}\n",
      "  - unique_col: unique  | meta={'rule': 'exact', 'match': 'unique'}\n",
      "----------------------------------------------------------------------\n",
      "Log columns in use:\n",
      "  - log_visits_auto (visits)\n",
      "  - log_unique_auto (unique)\n",
      "----------------------------------------------------------------------\n",
      "QC — Missingness (core cols):\n",
      "  - ct_id: 0\n",
      "  - mii: 0\n",
      "  - visits: 0\n",
      "  - unique: 0\n",
      "  - log_visits_auto: 0\n",
      "  - log_unique_auto: 0\n",
      "----------------------------------------------------------------------\n",
      "QC — Ranges (key variables):\n",
      "  - mii: min=-7.390038052969639, p50=0.19945636128961813, p99=0.804057941329089, max=1.6566794308497006\n",
      "  - visits: min=0.0, p50=395347.32482286077, p99=6721764.89978987, max=178209870.8740615\n",
      "  - unique: min=0.0, p50=68749.66887820396, p99=1973654.640624969, max=29887307.474513825\n",
      "  - log_visits_auto: min=0.0, p50=12.887522490289754, p99=15.720861460482608, max=18.99847246918859\n",
      "  - log_unique_auto: min=0.0, p50=11.138241744611378, p99=14.49539803618736, max=17.212944482514512\n",
      "----------------------------------------------------------------------\n",
      "Correlation diagnostics:\n",
      "  Pearson corr(MII, log(visits))  = 0.9500\n",
      "  Spearman corr(MII, log(visits)) = 0.9523\n",
      "----------------------------------------------------------------------\n",
      "Incremental explanation of MII by volume:\n",
      "                          model  n_obs       r2   adj_r2  beta_log_visits  p_log_visits  beta_log_unique  p_log_unique\n",
      "              MII ~ log(visits) 436868 0.902515 0.902514         0.335847           0.0              NaN           NaN\n",
      "MII ~ log(visits) + log(unique) 436868 0.914624 0.914624         0.529594           0.0        -0.207844           0.0\n",
      "----------------------------------------------------------------------\n",
      "Quadrant distribution:\n",
      "  - High volume / High infrastructure: 197,333\n",
      "  - Low volume / Low infrastructure: 197,333\n",
      "  - Low volume / High infrastructure: 21,101\n",
      "  - High volume / Low infrastructure: 21,101\n",
      "----------------------------------------------------------------------\n",
      "✅ Block V1 completed successfully.\n",
      "Outputs saved in: /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/\n",
      "Runtime: 6.91 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Block V1 — Distinctiveness: MII ≠ Volume (Robust Column Mapping)\n",
    "# Paper 3 — Pipeline V (Validation)\n",
    "#\n",
    "# PURPOSE\n",
    "# Demonstrate that the Mobility Infrastructure Index (MII)\n",
    "# is not reducible to simple mobility volume measures.\n",
    "#\n",
    "# INPUT\n",
    "# - Output from Pipeline A4 (Mobility + MII)\n",
    "#\n",
    "# OUTPUTS (saved to Desktop/Output Pipeline V (Validation))\n",
    "# - V1_mii_vs_volume_metrics.json\n",
    "# - V1_mii_vs_volume_models.csv\n",
    "# - V1_quadrant_typology_by_tract.csv.gz\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# -----------------------------\n",
    "# 0) PATHS (FINAL)\n",
    "# -----------------------------\n",
    "MII_INPUT_PATH = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline A (Mobility)/A4/\"\n",
    "    \"mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\"\n",
    ")\n",
    "\n",
    "V_OUT_DIR = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline V (Validation)/\"\n",
    ")\n",
    "\n",
    "os.makedirs(V_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) HELPERS — ROBUST COLUMN RESOLUTION\n",
    "# -----------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    return s.strip().lower()\n",
    "\n",
    "def _pick_first(existing_cols, preferred_exact, preferred_contains):\n",
    "    \"\"\"\n",
    "    Choose a column deterministically:\n",
    "    1) exact match (case-insensitive)\n",
    "    2) contains match (case-insensitive)\n",
    "    \"\"\"\n",
    "    norm_map = {_norm(c): c for c in existing_cols}\n",
    "\n",
    "    # exact\n",
    "    for cand in preferred_exact:\n",
    "        if _norm(cand) in norm_map:\n",
    "            return norm_map[_norm(cand)], {\"rule\": \"exact\", \"match\": cand}\n",
    "\n",
    "    # contains\n",
    "    existing_norm = [_norm(c) for c in existing_cols]\n",
    "    for pattern in preferred_contains:\n",
    "        pattern_n = _norm(pattern)\n",
    "        for i, c_n in enumerate(existing_norm):\n",
    "            if pattern_n in c_n:\n",
    "                return existing_cols[i], {\"rule\": \"contains\", \"match\": pattern}\n",
    "\n",
    "    return None, {\"rule\": None, \"match\": None}\n",
    "\n",
    "def resolve_volume_columns(df_cols):\n",
    "    \"\"\"\n",
    "    Robustly resolve:\n",
    "    - ct_id\n",
    "    - mii\n",
    "    - visits (raw volume)\n",
    "    - unique (raw unique visitors)\n",
    "    \"\"\"\n",
    "    # ct_id and mii are stable by your Pipeline S code\n",
    "    ct_id_col, ct_meta = _pick_first(\n",
    "        df_cols,\n",
    "        preferred_exact=[\"ct_id\"],\n",
    "        preferred_contains=[\"ct_id\", \"cd_setor\"]\n",
    "    )\n",
    "\n",
    "    mii_col, mii_meta = _pick_first(\n",
    "        df_cols,\n",
    "        preferred_exact=[\"mii\"],\n",
    "        preferred_contains=[\"mii\"]\n",
    "    )\n",
    "\n",
    "    # Visits candidates: be conservative (prefer totals over rates)\n",
    "    visits_col, visits_meta = _pick_first(\n",
    "        df_cols,\n",
    "        preferred_exact=[\n",
    "            \"total_visits\", \"visits\", \"n_visits\", \"visits_total\"\n",
    "        ],\n",
    "        preferred_contains=[\n",
    "            \"total_visits\", \"visits_total\", \"visits\", \"nvisits\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Unique candidates: prefer \"unique\" / \"unique_visitors\" / \"visitors_unique\"\n",
    "    unique_col, unique_meta = _pick_first(\n",
    "        df_cols,\n",
    "        preferred_exact=[\n",
    "            \"unique_visitors\", \"unique\", \"visitors_unique\", \"n_unique\", \"unique_users\"\n",
    "        ],\n",
    "        preferred_contains=[\n",
    "            \"unique_visitors\", \"visitors_unique\", \"unique\", \"uniq\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"ct_id_col\": ct_id_col, \"ct_id_meta\": ct_meta,\n",
    "        \"mii_col\": mii_col, \"mii_meta\": mii_meta,\n",
    "        \"visits_col\": visits_col, \"visits_meta\": visits_meta,\n",
    "        \"unique_col\": unique_col, \"unique_meta\": unique_meta,\n",
    "    }\n",
    "\n",
    "def list_candidates(df_cols, keyword):\n",
    "    kw = keyword.lower()\n",
    "    return [c for c in df_cols if kw in c.lower()]\n",
    "\n",
    "# -----------------------------\n",
    "# 2) INIT\n",
    "# -----------------------------\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "t0 = time.time()\n",
    "\n",
    "print(\"▶️ Pipeline V — Block V1: Distinctiveness (MII ≠ Volume)\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Input file: {MII_INPUT_PATH}\")\n",
    "print(f\"Output dir: {V_OUT_DIR}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) LOAD\n",
    "# -----------------------------\n",
    "df = pd.read_csv(MII_INPUT_PATH, compression=\"gzip\", low_memory=False)\n",
    "\n",
    "print(f\"Rows loaded: {len(df):,}\")\n",
    "print(f\"Columns loaded: {df.shape[1]:,}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) RESOLVE COLUMNS (ROBUST)\n",
    "# -----------------------------\n",
    "cols = df.columns.tolist()\n",
    "mapping = resolve_volume_columns(cols)\n",
    "\n",
    "print(\"Resolved column mapping (auto):\")\n",
    "for k in [\"ct_id_col\", \"mii_col\", \"visits_col\", \"unique_col\"]:\n",
    "    print(f\"  - {k}: {mapping[k]}  | meta={mapping[k.replace('_col','_meta')]}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "missing_core = [k for k in [\"ct_id_col\", \"mii_col\", \"visits_col\", \"unique_col\"] if mapping[k] is None]\n",
    "if missing_core:\n",
    "    print(\"❌ Could not resolve required columns automatically.\")\n",
    "    print(\"Column diagnostics (candidates):\")\n",
    "    print(f\"  - columns containing 'visit':  {list_candidates(cols, 'visit')[:50]}\")\n",
    "    print(f\"  - columns containing 'unique': {list_candidates(cols, 'unique')[:50]}\")\n",
    "    print(f\"  - columns containing 'visitor':{list_candidates(cols, 'visitor')[:50]}\")\n",
    "    print(f\"  - columns containing 'ct':     {list_candidates(cols, 'ct')[:50]}\")\n",
    "    raise ValueError(\n",
    "        f\"Missing resolved columns: {missing_core}. \"\n",
    "        \"Please inspect the candidate lists printed above.\"\n",
    "    )\n",
    "\n",
    "CT_ID_COL = mapping[\"ct_id_col\"]\n",
    "MII_COL = mapping[\"mii_col\"]\n",
    "VISITS_COL = mapping[\"visits_col\"]\n",
    "UNIQUE_COL = mapping[\"unique_col\"]\n",
    "\n",
    "# Log columns: prefer existing if present; otherwise create.\n",
    "# We will look for common log aliases first.\n",
    "LOG_VISITS_COL = None\n",
    "LOG_UNIQUE_COL = None\n",
    "\n",
    "# Try to find existing logs\n",
    "for cand in [\"log_total_visits\", \"log_visits\", \"log1p_total_visits\", \"log1p_visits\", \"log_visits_total\"]:\n",
    "    if cand in df.columns:\n",
    "        LOG_VISITS_COL = cand\n",
    "        break\n",
    "\n",
    "for cand in [\"log_unique_visitors\", \"log_unique\", \"log1p_unique_visitors\", \"log1p_unique\", \"log_visitors_unique\"]:\n",
    "    if cand in df.columns:\n",
    "        LOG_UNIQUE_COL = cand\n",
    "        break\n",
    "\n",
    "# Create logs if missing\n",
    "df[MII_COL] = pd.to_numeric(df[MII_COL], errors=\"coerce\")\n",
    "df[VISITS_COL] = pd.to_numeric(df[VISITS_COL], errors=\"coerce\")\n",
    "df[UNIQUE_COL] = pd.to_numeric(df[UNIQUE_COL], errors=\"coerce\")\n",
    "\n",
    "if LOG_VISITS_COL is None:\n",
    "    LOG_VISITS_COL = \"log_visits_auto\"\n",
    "    df[LOG_VISITS_COL] = np.log1p(df[VISITS_COL])\n",
    "\n",
    "if LOG_UNIQUE_COL is None:\n",
    "    LOG_UNIQUE_COL = \"log_unique_auto\"\n",
    "    df[LOG_UNIQUE_COL] = np.log1p(df[UNIQUE_COL])\n",
    "\n",
    "# Sanity checks\n",
    "for c in [LOG_VISITS_COL, LOG_UNIQUE_COL]:\n",
    "    if np.isinf(df[c]).any():\n",
    "        raise ValueError(f\"❌ Infinite values detected in {c}\")\n",
    "\n",
    "print(\"Log columns in use:\")\n",
    "print(f\"  - {LOG_VISITS_COL} (visits)\")\n",
    "print(f\"  - {LOG_UNIQUE_COL} (unique)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) QC — MISSINGNESS + RANGES\n",
    "# -----------------------------\n",
    "required_cols = [CT_ID_COL, MII_COL, VISITS_COL, UNIQUE_COL, LOG_VISITS_COL, LOG_UNIQUE_COL]\n",
    "qc_missing = {c: int(df[c].isna().sum()) for c in required_cols}\n",
    "\n",
    "print(\"QC — Missingness (core cols):\")\n",
    "for k, v in qc_missing.items():\n",
    "    print(f\"  - {k}: {v:,}\")\n",
    "\n",
    "def _safe_stats(s: pd.Series):\n",
    "    s2 = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s2.notna().sum() == 0:\n",
    "        return {\"min\": None, \"p50\": None, \"p99\": None, \"max\": None}\n",
    "    return {\n",
    "        \"min\": float(np.nanmin(s2)),\n",
    "        \"p50\": float(np.nanpercentile(s2, 50)),\n",
    "        \"p99\": float(np.nanpercentile(s2, 99)),\n",
    "        \"max\": float(np.nanmax(s2)),\n",
    "    }\n",
    "\n",
    "qc_ranges = {c: _safe_stats(df[c]) for c in [MII_COL, VISITS_COL, UNIQUE_COL, LOG_VISITS_COL, LOG_UNIQUE_COL]}\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"QC — Ranges (key variables):\")\n",
    "for c, st in qc_ranges.items():\n",
    "    print(f\"  - {c}: min={st['min']}, p50={st['p50']}, p99={st['p99']}, max={st['max']}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# 6) ANALYSIS A — CORRELATIONS\n",
    "# -----------------------------\n",
    "corr_vars = [MII_COL, VISITS_COL, UNIQUE_COL, LOG_VISITS_COL, LOG_UNIQUE_COL]\n",
    "corr_df = df[corr_vars].dropna()\n",
    "\n",
    "pearson_corr = corr_df.corr(method=\"pearson\")\n",
    "spearman_corr = corr_df.corr(method=\"spearman\")\n",
    "\n",
    "print(\"Correlation diagnostics:\")\n",
    "print(f\"  Pearson corr(MII, log(visits))  = {pearson_corr.loc[MII_COL, LOG_VISITS_COL]:.4f}\")\n",
    "print(f\"  Spearman corr(MII, log(visits)) = {spearman_corr.loc[MII_COL, LOG_VISITS_COL]:.4f}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) ANALYSIS B — HOW MUCH DOES VOLUME EXPLAIN MII?\n",
    "# -----------------------------\n",
    "reg_df = df[[MII_COL, LOG_VISITS_COL, LOG_UNIQUE_COL]].dropna()\n",
    "\n",
    "y = reg_df[MII_COL]\n",
    "X1 = sm.add_constant(reg_df[[LOG_VISITS_COL]])\n",
    "X2 = sm.add_constant(reg_df[[LOG_VISITS_COL, LOG_UNIQUE_COL]])\n",
    "\n",
    "m1 = sm.OLS(y, X1).fit()\n",
    "m2 = sm.OLS(y, X2).fit()\n",
    "\n",
    "models_out = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"MII ~ log(visits)\",\n",
    "        \"n_obs\": int(m1.nobs),\n",
    "        \"r2\": float(m1.rsquared),\n",
    "        \"adj_r2\": float(m1.rsquared_adj),\n",
    "        \"beta_log_visits\": float(m1.params[LOG_VISITS_COL]),\n",
    "        \"p_log_visits\": float(m1.pvalues[LOG_VISITS_COL]),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"MII ~ log(visits) + log(unique)\",\n",
    "        \"n_obs\": int(m2.nobs),\n",
    "        \"r2\": float(m2.rsquared),\n",
    "        \"adj_r2\": float(m2.rsquared_adj),\n",
    "        \"beta_log_visits\": float(m2.params[LOG_VISITS_COL]),\n",
    "        \"p_log_visits\": float(m2.pvalues[LOG_VISITS_COL]),\n",
    "        \"beta_log_unique\": float(m2.params[LOG_UNIQUE_COL]),\n",
    "        \"p_log_unique\": float(m2.pvalues[LOG_UNIQUE_COL]),\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Incremental explanation of MII by volume:\")\n",
    "print(models_out.to_string(index=False))\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) QUADRANT TYPOLOGY — VOLUME × INFRASTRUCTURE\n",
    "# -----------------------------\n",
    "quad_df = df[[CT_ID_COL, MII_COL, LOG_VISITS_COL]].dropna()\n",
    "\n",
    "mii_median = quad_df[MII_COL].median()\n",
    "vol_median = quad_df[LOG_VISITS_COL].median()\n",
    "\n",
    "def assign_quadrant(row):\n",
    "    if row[MII_COL] >= mii_median and row[LOG_VISITS_COL] >= vol_median:\n",
    "        return \"High volume / High infrastructure\"\n",
    "    if row[MII_COL] < mii_median and row[LOG_VISITS_COL] >= vol_median:\n",
    "        return \"High volume / Low infrastructure\"\n",
    "    if row[MII_COL] >= mii_median and row[LOG_VISITS_COL] < vol_median:\n",
    "        return \"Low volume / High infrastructure\"\n",
    "    return \"Low volume / Low infrastructure\"\n",
    "\n",
    "quad_df[\"quadrant\"] = quad_df.apply(assign_quadrant, axis=1)\n",
    "\n",
    "quad_counts = quad_df[\"quadrant\"].value_counts().to_dict()\n",
    "\n",
    "print(\"Quadrant distribution:\")\n",
    "for k, v in quad_counts.items():\n",
    "    print(f\"  - {k}: {v:,}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# -----------------------------\n",
    "# 9) SAVE OUTPUTS\n",
    "# -----------------------------\n",
    "metrics = {\n",
    "    \"run_id\": run_id,\n",
    "    \"input_path\": MII_INPUT_PATH,\n",
    "    \"rows_total\": int(len(df)),\n",
    "    \"resolved_columns\": {\n",
    "        \"ct_id\": CT_ID_COL,\n",
    "        \"mii\": MII_COL,\n",
    "        \"visits\": VISITS_COL,\n",
    "        \"unique\": UNIQUE_COL,\n",
    "        \"log_visits\": LOG_VISITS_COL,\n",
    "        \"log_unique\": LOG_UNIQUE_COL,\n",
    "        \"resolution_meta\": {\n",
    "            \"ct_id\": mapping[\"ct_id_meta\"],\n",
    "            \"mii\": mapping[\"mii_meta\"],\n",
    "            \"visits\": mapping[\"visits_meta\"],\n",
    "            \"unique\": mapping[\"unique_meta\"],\n",
    "        }\n",
    "    },\n",
    "    \"missing_required\": qc_missing,\n",
    "    \"ranges\": qc_ranges,\n",
    "    \"pearson_corr\": {\"mii_log_visits\": float(pearson_corr.loc[MII_COL, LOG_VISITS_COL])},\n",
    "    \"spearman_corr\": {\"mii_log_visits\": float(spearman_corr.loc[MII_COL, LOG_VISITS_COL])},\n",
    "    \"models\": models_out.to_dict(orient=\"records\"),\n",
    "    \"quadrant_counts\": quad_counts,\n",
    "    \"runtime_seconds\": round(time.time() - t0, 2),\n",
    "}\n",
    "\n",
    "with open(os.path.join(V_OUT_DIR, \"V1_mii_vs_volume_metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "models_out.to_csv(os.path.join(V_OUT_DIR, \"V1_mii_vs_volume_models.csv\"), index=False)\n",
    "\n",
    "quad_df.to_csv(\n",
    "    os.path.join(V_OUT_DIR, \"V1_quadrant_typology_by_tract.csv.gz\"),\n",
    "    index=False,\n",
    "    compression=\"gzip\"\n",
    ")\n",
    "\n",
    "print(\"✅ Block V1 completed successfully.\")\n",
    "print(f\"Outputs saved in: {V_OUT_DIR}\")\n",
    "print(f\"Runtime: {metrics['runtime_seconds']} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4dfb5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved component mapping:\n",
      "  - mii: mii\n",
      "  - visits: visits\n",
      "  - unique: unique\n",
      "  - dwell: dwell_time_mins\n",
      "  - repeat: repeat_visitors\n",
      "  - new: new_visitors\n",
      "  - stability: stability_visits_week_cv_A4\n",
      "✅ Block V2 completed successfully.\n",
      "Outputs saved in: /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/V2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline V — Block V2\n",
    "# Ablation & Internal Validity of the MII (Robust Version)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------------------\n",
    "INPUT_PATH = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline A (Mobility)/A4/\"\n",
    "    \"mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\"\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline V (Validation)/V2\"\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_csv(INPUT_PATH, compression=\"gzip\", low_memory=False)\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HELPER — COLUMN RESOLUTION (CONCEPT-DRIVEN)\n",
    "# ------------------------------------------------------------\n",
    "def find_col(preferred_exact, preferred_contains):\n",
    "    for c in preferred_exact:\n",
    "        if c in cols:\n",
    "            return c, {\"rule\": \"exact\", \"match\": c}\n",
    "    for pat in preferred_contains:\n",
    "        for c in cols:\n",
    "            if pat.lower() in c.lower():\n",
    "                return c, {\"rule\": \"contains\", \"match\": pat}\n",
    "    return None, None\n",
    "\n",
    "resolved = {}\n",
    "\n",
    "# Core\n",
    "resolved[\"mii\"], _ = find_col([\"mii\"], [\"mii\"])\n",
    "resolved[\"visits\"], _ = find_col(\n",
    "    [\"visits\", \"total_visits\"],\n",
    "    [\"visits\"]\n",
    ")\n",
    "resolved[\"unique\"], _ = find_col(\n",
    "    [\"unique\", \"unique_visitors\"],\n",
    "    [\"unique\"]\n",
    ")\n",
    "\n",
    "# Permanence\n",
    "resolved[\"dwell\"], _ = find_col(\n",
    "    [\"dwell_time\", \"avg_dwell_time\", \"mean_dwell_time\"],\n",
    "    [\"dwell\"]\n",
    ")\n",
    "\n",
    "# Recurrence\n",
    "resolved[\"repeat\"], _ = find_col(\n",
    "    [\"repeat_visits\", \"repeat\"],\n",
    "    [\"repeat\"]\n",
    ")\n",
    "resolved[\"new\"], _ = find_col(\n",
    "    [\"new_visitors\", \"new\"],\n",
    "    [\"new\"]\n",
    ")\n",
    "\n",
    "# Stability\n",
    "resolved[\"stability\"], _ = find_col(\n",
    "    [\"stability_cv\", \"cv_weekly\", \"weekly_cv\"],\n",
    "    [\"stability\", \"cv\"]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CHECK RESOLUTION\n",
    "# ------------------------------------------------------------\n",
    "missing = [k for k, v in resolved.items() if v is None and k != \"mii\"]\n",
    "\n",
    "print(\"Resolved component mapping:\")\n",
    "for k, v in resolved.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "if resolved[\"mii\"] is None:\n",
    "    raise ValueError(\"❌ Column 'mii' not found — Pipeline A4b integrity violated.\")\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n❌ Missing conceptual components:\")\n",
    "    for k in missing:\n",
    "        print(f\"  - {k}\")\n",
    "    print(\"\\nAvailable columns:\")\n",
    "    print(cols)\n",
    "    raise ValueError(\"Ablation cannot proceed without resolving conceptual components.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HELPER — Z-MEAN\n",
    "# ------------------------------------------------------------\n",
    "def zmean(cols):\n",
    "    Z = StandardScaler().fit_transform(df[cols])\n",
    "    return Z.mean(axis=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONSTRUCT ABLATION INDICES\n",
    "# ------------------------------------------------------------\n",
    "df[\"idx_volume_only\"] = zmean([resolved[\"visits\"]])\n",
    "\n",
    "df[\"idx_volume_unique\"] = zmean([\n",
    "    resolved[\"visits\"],\n",
    "    resolved[\"unique\"]\n",
    "])\n",
    "\n",
    "df[\"idx_volume_dwell\"] = zmean([\n",
    "    resolved[\"visits\"],\n",
    "    resolved[\"dwell\"]\n",
    "])\n",
    "\n",
    "df[\"idx_no_stability\"] = zmean([\n",
    "    resolved[\"visits\"],\n",
    "    resolved[\"unique\"],\n",
    "    resolved[\"dwell\"],\n",
    "    resolved[\"repeat\"],\n",
    "    resolved[\"new\"]\n",
    "])\n",
    "\n",
    "df[\"idx_no_recurrence\"] = zmean([\n",
    "    resolved[\"visits\"],\n",
    "    resolved[\"unique\"],\n",
    "    resolved[\"dwell\"],\n",
    "    resolved[\"stability\"]\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CORRELATION WITH TRUE MII\n",
    "# ------------------------------------------------------------\n",
    "indices = [\n",
    "    \"idx_volume_only\",\n",
    "    \"idx_volume_unique\",\n",
    "    \"idx_volume_dwell\",\n",
    "    \"idx_no_stability\",\n",
    "    \"idx_no_recurrence\"\n",
    "]\n",
    "\n",
    "corr_results = []\n",
    "for idx in indices:\n",
    "    rho, _ = spearmanr(df[idx], df[resolved[\"mii\"]])\n",
    "    corr_results.append({\n",
    "        \"index\": idx,\n",
    "        \"spearman_corr_with_mii\": round(float(rho), 4)\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_results)\n",
    "corr_df.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"V2_ablation_correlations.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RANK STABILITY — TOP 10%\n",
    "# ------------------------------------------------------------\n",
    "def top_overlap(a, b, q=0.9):\n",
    "    ta = set(df.loc[df[a] >= df[a].quantile(q)].index)\n",
    "    tb = set(df.loc[df[b] >= df[b].quantile(q)].index)\n",
    "    return len(ta & tb) / len(ta)\n",
    "\n",
    "rank_results = []\n",
    "for idx in indices:\n",
    "    overlap = top_overlap(resolved[\"mii\"], idx)\n",
    "    rank_results.append({\n",
    "        \"index\": idx,\n",
    "        \"top10_overlap_with_mii\": round(float(overlap), 4)\n",
    "    })\n",
    "\n",
    "rank_df = pd.DataFrame(rank_results)\n",
    "rank_df.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"V2_rank_stability.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# QC REPORT\n",
    "# ------------------------------------------------------------\n",
    "qc = {\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"resolved_columns\": resolved,\n",
    "    \"indices_tested\": indices,\n",
    "    \"interpretation_rule\": (\n",
    "        \"Low correlation and low top-rank overlap for simplified indices \"\n",
    "        \"indicate that the full infrastructural architecture of the MII \"\n",
    "        \"is empirically necessary.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"V2_ablation_qc.json\"), \"w\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "print(\"✅ Block V2 completed successfully.\")\n",
    "print(f\"Outputs saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7010513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Block V3 completed successfully.\n",
      "Outputs saved in: /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/V3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline V — Block V3\n",
    "# Reliability & Stability of the Mobility Infrastructure Index\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------------------\n",
    "INPUT_PATH = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline A (Mobility)/A4/\"\n",
    "    \"mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\"\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline V (Validation)/V3\"\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_csv(INPUT_PATH, compression=\"gzip\", low_memory=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# COMPONENTS (already validated in V2)\n",
    "# ------------------------------------------------------------\n",
    "components = {\n",
    "    \"visits\": \"visits\",\n",
    "    \"unique\": \"unique\",\n",
    "    \"dwell\": \"dwell_time_mins\",\n",
    "    \"repeat\": \"repeat_visitors\",\n",
    "    \"new\": \"new_visitors\",\n",
    "    \"stability\": \"stability_visits_week_cv_A4\"\n",
    "}\n",
    "\n",
    "for k, c in components.items():\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Required component missing: {c}\")\n",
    "\n",
    "if \"mii\" not in df.columns:\n",
    "    raise ValueError(\"Column 'mii' not found.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def zmean(cols):\n",
    "    Z = StandardScaler().fit_transform(df[cols])\n",
    "    return Z.mean(axis=1)\n",
    "\n",
    "def top_overlap(a, b, q=0.9):\n",
    "    ta = set(df.loc[df[a] >= df[a].quantile(q)].index)\n",
    "    tb = set(df.loc[df[b] >= df[b].quantile(q)].index)\n",
    "    return len(ta & tb) / len(ta)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BASELINE (FULL STRUCTURE, NO PCA)\n",
    "# ------------------------------------------------------------\n",
    "full_cols = list(components.values())\n",
    "df[\"mii_zmean_full\"] = zmean(full_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PERTURBATION TESTS — DROP ONE COMPONENT\n",
    "# ------------------------------------------------------------\n",
    "results_corr = []\n",
    "results_rank = []\n",
    "\n",
    "for drop_key, drop_col in components.items():\n",
    "    cols_used = [c for c in full_cols if c != drop_col]\n",
    "    label = f\"drop_{drop_key}\"\n",
    "\n",
    "    df[label] = zmean(cols_used)\n",
    "\n",
    "    rho, _ = spearmanr(df[\"mii\"], df[label])\n",
    "    overlap = top_overlap(\"mii\", label)\n",
    "\n",
    "    results_corr.append({\n",
    "        \"perturbation\": label,\n",
    "        \"spearman_corr_with_mii\": round(float(rho), 4)\n",
    "    })\n",
    "\n",
    "    results_rank.append({\n",
    "        \"perturbation\": label,\n",
    "        \"top10_overlap_with_mii\": round(float(overlap), 4)\n",
    "    })\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# GLOBAL RELIABILITY SUMMARY\n",
    "# ------------------------------------------------------------\n",
    "summary = {\n",
    "    \"mean_spearman_corr\": float(\n",
    "        np.mean([r[\"spearman_corr_with_mii\"] for r in results_corr])\n",
    "    ),\n",
    "    \"min_spearman_corr\": float(\n",
    "        np.min([r[\"spearman_corr_with_mii\"] for r in results_corr])\n",
    "    ),\n",
    "    \"mean_top10_overlap\": float(\n",
    "        np.mean([r[\"top10_overlap_with_mii\"] for r in results_rank])\n",
    "    ),\n",
    "    \"min_top10_overlap\": float(\n",
    "        np.min([r[\"top10_overlap_with_mii\"] for r in results_rank])\n",
    "    ),\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SAVE OUTPUTS\n",
    "# ------------------------------------------------------------\n",
    "corr_df = pd.DataFrame(results_corr)\n",
    "corr_df.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"V3_mii_perturbation_correlations.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "rank_df = pd.DataFrame(results_rank)\n",
    "rank_df.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"V3_mii_rank_stability.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "qc = {\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"components\": components,\n",
    "    \"perturbations_tested\": list(components.keys()),\n",
    "    \"summary\": summary,\n",
    "    \"interpretation_rule\": (\n",
    "        \"High correlation and rank overlap under component perturbation \"\n",
    "        \"indicate that the MII behaves as a stable infrastructural signal \"\n",
    "        \"rather than a fragile composite.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"V3_mii_reliability_qc.json\"), \"w\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "print(\"✅ Block V3 completed successfully.\")\n",
    "print(f\"Outputs saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db695720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing 27 CNEFE ZIP files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UF ZIPs: 100%|██████████| 27/27 [10:33<00:00, 23.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Block V4 completed successfully.\n",
      "Outputs saved in: /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/V4\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline V — Block V4\n",
    "# External Validity using CNEFE (IBGE 2022)\n",
    "# Robust version with tqdm and safe overlap\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ------------------------------------------------------------\n",
    "MII_PATH = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline A (Mobility)/A4/\"\n",
    "    \"mobility_by_tract_aug2024_with_mii_FINAL.csv.gz\"\n",
    ")\n",
    "\n",
    "CNEFE_DIR = \"/Users/rafaelalbuquerque/Desktop/Arquivos_CNEFE\"\n",
    "\n",
    "OUTPUT_DIR = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline V (Validation)/V4\"\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD MII\n",
    "# ------------------------------------------------------------\n",
    "mii_df = pd.read_csv(\n",
    "    MII_PATH,\n",
    "    compression=\"gzip\",\n",
    "    dtype={\"ct_id\": str},\n",
    "    usecols=[\"ct_id\", \"mii\"]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ------------------------------------------------------------\n",
    "def resolve_ct_col(columns):\n",
    "    \"\"\"\n",
    "    Resolve census tract id column semantically.\n",
    "    \"\"\"\n",
    "    for c in columns:\n",
    "        cl = c.lower()\n",
    "        if (\"setor\" in cl) and (\"cd\" in cl or \"cod\" in cl):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def try_read(file_obj):\n",
    "    \"\"\"\n",
    "    Try multiple separators; return df or None.\n",
    "    \"\"\"\n",
    "    for sep in [\";\", \"|\", \"\\t\", \",\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_obj,\n",
    "                sep=sep,\n",
    "                dtype=str,\n",
    "                low_memory=False\n",
    "            )\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def top_overlap_safe(df, a, b, q=0.9):\n",
    "    \"\"\"\n",
    "    Safe top-q overlap (returns NaN if undefined).\n",
    "    \"\"\"\n",
    "    ta = set(df.loc[df[a] >= df[a].quantile(q)].index)\n",
    "    tb = set(df.loc[df[b] >= df[b].quantile(q)].index)\n",
    "\n",
    "    if len(ta) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return len(ta & tb) / len(ta)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD + AGGREGATE CNEFE\n",
    "# ------------------------------------------------------------\n",
    "cnefe_counts = {}\n",
    "\n",
    "zip_files = sorted([f for f in os.listdir(CNEFE_DIR) if f.endswith(\".zip\")])\n",
    "\n",
    "print(f\"[INFO] Processing {len(zip_files)} CNEFE ZIP files...\")\n",
    "\n",
    "for z in tqdm(zip_files, desc=\"UF ZIPs\"):\n",
    "    zpath = os.path.join(CNEFE_DIR, z)\n",
    "\n",
    "    with zipfile.ZipFile(zpath, \"r\") as zipf:\n",
    "        inner_files = [\n",
    "            f for f in zipf.namelist()\n",
    "            if f.lower().endswith((\".csv\", \".txt\", \".dat\"))\n",
    "        ]\n",
    "\n",
    "        for fname in inner_files:\n",
    "            with zipf.open(fname) as f:\n",
    "                df = try_read(f)\n",
    "                if df is None:\n",
    "                    continue\n",
    "\n",
    "                ct_col = resolve_ct_col(df.columns)\n",
    "                if ct_col is None:\n",
    "                    continue\n",
    "\n",
    "                # aggregate counts\n",
    "                for ct in df[ct_col].dropna():\n",
    "                    cnefe_counts[ct] = cnefe_counts.get(ct, 0) + 1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BUILD AGGREGATED DATAFRAME\n",
    "# ------------------------------------------------------------\n",
    "if not cnefe_counts:\n",
    "    raise ValueError(\n",
    "        \"❌ No CNEFE records aggregated. \"\n",
    "        \"Check file formats or column naming.\"\n",
    "    )\n",
    "\n",
    "cnefe_df = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        cnefe_counts,\n",
    "        orient=\"index\",\n",
    "        columns=[\"n_addresses\"]\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"ct_id\"})\n",
    ")\n",
    "\n",
    "cnefe_df.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"V4_cnefe_aggregation_by_tract.csv.gz\"),\n",
    "    index=False,\n",
    "    compression=\"gzip\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MERGE WITH MII\n",
    "# ------------------------------------------------------------\n",
    "merged = mii_df.merge(cnefe_df, on=\"ct_id\", how=\"inner\")\n",
    "merged[\"log_addresses\"] = np.log1p(merged[\"n_addresses\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CORRELATION TESTS\n",
    "# ------------------------------------------------------------\n",
    "rho_raw, _ = spearmanr(merged[\"mii\"], merged[\"n_addresses\"])\n",
    "rho_log, _ = spearmanr(merged[\"mii\"], merged[\"log_addresses\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TOP 10% OVERLAP (SAFE)\n",
    "# ------------------------------------------------------------\n",
    "overlap_raw = top_overlap_safe(merged, \"mii\", \"n_addresses\", q=0.9)\n",
    "overlap_log = top_overlap_safe(merged, \"mii\", \"log_addresses\", q=0.9)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SAVE RESULTS\n",
    "# ------------------------------------------------------------\n",
    "results = pd.DataFrame([\n",
    "    {\n",
    "        \"metric\": \"addresses_raw\",\n",
    "        \"spearman_corr\": round(float(rho_raw), 4),\n",
    "        \"top10_overlap\": (\n",
    "            None if np.isnan(overlap_raw) else round(float(overlap_raw), 4)\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"metric\": \"addresses_log\",\n",
    "        \"spearman_corr\": round(float(rho_log), 4),\n",
    "        \"top10_overlap\": (\n",
    "            None if np.isnan(overlap_log) else round(float(overlap_log), 4)\n",
    "        ),\n",
    "    },\n",
    "])\n",
    "\n",
    "results.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"V4_mii_vs_cnefe_correlations.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "qc = {\n",
    "    \"n_mii_tracts\": int(len(mii_df)),\n",
    "    \"n_cnefe_tracts\": int(len(cnefe_df)),\n",
    "    \"n_merged\": int(len(merged)),\n",
    "    \"zip_files_processed\": zip_files,\n",
    "    \"overlap_definition\": \"Top-q overlap; undefined (NaN) if no tract exceeds q-quantile\",\n",
    "    \"interpretation_rule\": (\n",
    "        \"A positive monotonic association between MII and \"\n",
    "        \"CNEFE-based address density supports external validity \"\n",
    "        \"of the index as a proxy for market infrastructure.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"V4_mii_vs_cnefe_qc.json\"), \"w\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "print(\"✅ Block V4 completed successfully.\")\n",
    "print(f\"Outputs saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad94561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Block V4b starting — External validity via nightlights\n",
      " - Tracts GPKG : /Users/rafaelalbuquerque/Desktop/Output Pipeline S (Shapefiles)/S2/census_tracts_brazil_mobility_mii.gpkg\n",
      " - Raster     : /Users/rafaelalbuquerque/Desktop/Nightlights/VIIRS_annual_2024.tif\n",
      " - Output dir : /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/V4b\n",
      "\n",
      "[STEP 1/5] Loading census tracts with MII...\n",
      "[INFO] Tracts loaded: 472,780\n",
      "[INFO] CRS (tracts): EPSG:4674\n",
      "\n",
      "[STEP 2/5] Opening nightlights raster...\n",
      "[INFO] Raster CRS: GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST]]\n",
      "[INFO] Reprojecting tracts to raster CRS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: internal_proj_identify: /opt/anaconda3/share/proj/proj.db contains DATABASE.LAYOUT.VERSION.MINOR = 2 whereas a number >= 6 is expected. It comes from another PROJ installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3/5] Computing zonal statistics (mean, median, count)...\n",
      "\n",
      "[STEP 4/5] QC and preparation...\n",
      "[INFO] Tracts total          : 472,780\n",
      "[INFO] Tracts with NL signal : 472,780\n",
      "[INFO] Tracts with zero pix  : 0\n",
      "\n",
      "[STEP 5/5] Correlation tests...\n",
      "\n",
      "[SAVE] Writing outputs...\n",
      "\n",
      "[DONE] Block V4b completed successfully.\n",
      " - Zonal stats : /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/V4b/V4b_nightlights_by_tract.csv.gz\n",
      " - Correlation : /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/V4b/V4b_mii_vs_nightlights_correlations.csv\n",
      " - QC report  : /Users/rafaelalbuquerque/Desktop/Output Pipeline V (Validation)/V4b/V4b_mii_vs_nightlights_qc.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Pipeline V — Block V4b\n",
    "# External Validity using VIIRS Nighttime Lights (Annual, masked)\n",
    "# Zonal statistics at census tract level\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from exactextract import exact_extract\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "TRACTS_GPKG = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline S (Shapefiles)/S2/\"\n",
    "    \"census_tracts_brazil_mobility_mii.gpkg\"\n",
    ")\n",
    "\n",
    "NIGHTLIGHTS_TIF = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Nightlights/\"\n",
    "    \"VIIRS_annual_2024.tif\"\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = (\n",
    "    \"/Users/rafaelalbuquerque/Desktop/\"\n",
    "    \"Output Pipeline V (Validation)/V4b\"\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_ZONAL = os.path.join(\n",
    "    OUTPUT_DIR, \"V4b_nightlights_by_tract.csv.gz\"\n",
    ")\n",
    "OUT_CORR = os.path.join(\n",
    "    OUTPUT_DIR, \"V4b_mii_vs_nightlights_correlations.csv\"\n",
    ")\n",
    "OUT_QC = os.path.join(\n",
    "    OUTPUT_DIR, \"V4b_mii_vs_nightlights_qc.json\"\n",
    ")\n",
    "\n",
    "print(\"[INFO] Block V4b starting — External validity via nightlights\")\n",
    "print(f\" - Tracts GPKG : {TRACTS_GPKG}\")\n",
    "print(f\" - Raster     : {NIGHTLIGHTS_TIF}\")\n",
    "print(f\" - Output dir : {OUTPUT_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 1 — LOAD TRACTS WITH MII\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[STEP 1/5] Loading census tracts with MII...\")\n",
    "\n",
    "gdf = gpd.read_file(TRACTS_GPKG)\n",
    "\n",
    "required_cols = [\"ct_id\", \"mii\", \"geometry\"]\n",
    "missing = [c for c in required_cols if c not in gdf.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in GPKG: {missing}\")\n",
    "\n",
    "gdf[\"ct_id\"] = gdf[\"ct_id\"].astype(str)\n",
    "\n",
    "print(f\"[INFO] Tracts loaded: {len(gdf):,}\")\n",
    "print(f\"[INFO] CRS (tracts): {gdf.crs}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 2 — OPEN RASTER & ALIGN CRS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[STEP 2/5] Opening nightlights raster...\")\n",
    "\n",
    "with rasterio.open(NIGHTLIGHTS_TIF) as src:\n",
    "    raster_crs = src.crs\n",
    "    nodata = src.nodata\n",
    "\n",
    "print(f\"[INFO] Raster CRS: {raster_crs}\")\n",
    "\n",
    "if gdf.crs != raster_crs:\n",
    "    print(\"[INFO] Reprojecting tracts to raster CRS...\")\n",
    "    gdf = gdf.to_crs(raster_crs)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 3 — ZONAL STATISTICS (exactextract)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[STEP 3/5] Computing zonal statistics (mean, median, count)...\")\n",
    "\n",
    "stats = exact_extract(\n",
    "    NIGHTLIGHTS_TIF,\n",
    "    gdf,\n",
    "    [\"mean\", \"median\", \"count\"],\n",
    "    output=\"pandas\"\n",
    ")\n",
    "\n",
    "zonal_df = pd.DataFrame({\n",
    "    \"ct_id\": gdf[\"ct_id\"].values,\n",
    "    \"mii\": gdf[\"mii\"].values,\n",
    "    \"nl_mean\": stats[\"mean\"].values,\n",
    "    \"nl_median\": stats[\"median\"].values,\n",
    "    \"nl_count\": stats[\"count\"].values\n",
    "})\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 4 — QC & PREP\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[STEP 4/5] QC and preparation...\")\n",
    "\n",
    "zonal_df[\"nl_mean\"] = pd.to_numeric(zonal_df[\"nl_mean\"], errors=\"coerce\")\n",
    "zonal_df[\"nl_median\"] = pd.to_numeric(zonal_df[\"nl_median\"], errors=\"coerce\")\n",
    "zonal_df[\"nl_count\"] = pd.to_numeric(zonal_df[\"nl_count\"], errors=\"coerce\")\n",
    "\n",
    "n_total = len(zonal_df)\n",
    "n_valid = zonal_df[\"nl_mean\"].notna().sum()\n",
    "n_zero_pixels = (zonal_df[\"nl_count\"].fillna(0) == 0).sum()\n",
    "\n",
    "zonal_df = zonal_df[zonal_df[\"nl_mean\"].notna()].copy()\n",
    "zonal_df[\"log_nl_mean\"] = np.log1p(zonal_df[\"nl_mean\"])\n",
    "\n",
    "print(f\"[INFO] Tracts total          : {n_total:,}\")\n",
    "print(f\"[INFO] Tracts with NL signal : {n_valid:,}\")\n",
    "print(f\"[INFO] Tracts with zero pix  : {n_zero_pixels:,}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 5 — CORRELATION & OVERLAP\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[STEP 5/5] Correlation tests...\")\n",
    "\n",
    "rho_raw, _ = spearmanr(zonal_df[\"mii\"], zonal_df[\"nl_mean\"])\n",
    "rho_log, _ = spearmanr(zonal_df[\"mii\"], zonal_df[\"log_nl_mean\"])\n",
    "\n",
    "def top_overlap_safe(df, a, b, q=0.9):\n",
    "    ta = set(df.loc[df[a] >= df[a].quantile(q)].index)\n",
    "    tb = set(df.loc[df[b] >= df[b].quantile(q)].index)\n",
    "    if len(ta) == 0:\n",
    "        return np.nan\n",
    "    return len(ta & tb) / len(ta)\n",
    "\n",
    "overlap_raw = top_overlap_safe(zonal_df, \"mii\", \"nl_mean\", q=0.9)\n",
    "overlap_log = top_overlap_safe(zonal_df, \"mii\", \"log_nl_mean\", q=0.9)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SAVE OUTPUTS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[SAVE] Writing outputs...\")\n",
    "\n",
    "zonal_df.to_csv(\n",
    "    OUT_ZONAL,\n",
    "    index=False,\n",
    "    compression=\"gzip\"\n",
    ")\n",
    "\n",
    "corr_df = pd.DataFrame([\n",
    "    {\n",
    "        \"metric\": \"nightlights_raw\",\n",
    "        \"spearman_corr\": round(float(rho_raw), 4),\n",
    "        \"top10_overlap\": (\n",
    "            None if np.isnan(overlap_raw)\n",
    "            else round(float(overlap_raw), 4)\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"metric\": \"nightlights_log\",\n",
    "        \"spearman_corr\": round(float(rho_log), 4),\n",
    "        \"top10_overlap\": (\n",
    "            None if np.isnan(overlap_log)\n",
    "            else round(float(overlap_log), 4)\n",
    "        )\n",
    "    }\n",
    "])\n",
    "\n",
    "corr_df.to_csv(OUT_CORR, index=False)\n",
    "\n",
    "qc = {\n",
    "    \"engine_used\": \"exactextract\",\n",
    "    \"n_tracts_input\": int(n_total),\n",
    "    \"n_tracts_with_signal\": int(n_valid),\n",
    "    \"n_zero_pixel_count\": int(n_zero_pixels),\n",
    "    \"nightlights_raster\": NIGHTLIGHTS_TIF,\n",
    "    \"interpretation_rule\": (\n",
    "        \"Positive monotonic association between MII and \"\n",
    "        \"nighttime light intensity supports external validity \"\n",
    "        \"using an independent physical proxy of sustained \"\n",
    "        \"spatial activity.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "with open(OUT_QC, \"w\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "print(\"\\n[DONE] Block V4b completed successfully.\")\n",
    "print(f\" - Zonal stats : {OUT_ZONAL}\")\n",
    "print(f\" - Correlation : {OUT_CORR}\")\n",
    "print(f\" - QC report  : {OUT_QC}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
